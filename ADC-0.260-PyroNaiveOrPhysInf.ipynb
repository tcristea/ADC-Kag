{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101849,"databundleVersionId":13093295,"sourceType":"competition"},{"sourceId":12303949,"sourceType":"datasetVersion","datasetId":7755350},{"sourceId":12309150,"sourceType":"datasetVersion","datasetId":7758622}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":46.011924,"end_time":"2025-06-28T07:57:11.831616","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-28T07:56:25.819692","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install pyro-ppl[extras]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyro\nimport torch\nfrom pyro.infer.autoguide import AutoMultivariateNormal, AutoDelta\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam as PyroAdam\n\ndef setup_pyro_model():\n    \"\"\"Complete setup for Pyro model with proper error handling.\"\"\"\n    \n    # Clear parameter store\n    pyro.clear_param_store()\n    \n    # Create model\n    model = PhysicsInformedPyroModel(\n        X_train_tensor.shape[1], \n        y_train_tensor.shape[1],\n        feature_names=feature_names,\n        wavelengths=wavelengths\n    ).to(device)\n    \n    print(\"=== Attempting AutoMultivariateNormal ===\")\n    \n    # Try AutoMultivariateNormal first\n    guide = AutoMultivariateNormal(model)\n    \n    # Initialize guide properly\n    sample_x = X_train_tensor[:2].to(device)\n    sample_y = y_train_tensor[:2].to(device)\n    \n    # Run model to initialize guide\n    with torch.no_grad():\n        model(sample_x, sample_y)\n    \n    # Force guide initialization by calling it\n    with torch.no_grad():\n        guide(sample_x, sample_y)\n    \n    guide_params = list(guide.parameters())\n    print(f\"AutoMultivariateNormal parameters: {len(guide_params)}\")\n    \n    if len(guide_params) > 0:\n        # Use Pyro optimizer (required for SVI)\n        optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n        svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n        \n        print(\"‚úì AutoMultivariateNormal setup successful!\")\n        return model, guide, svi, \"AutoMultivariateNormal\"\n    \n    print(\"=== Fallback to AutoDelta ===\")\n    \n    # Fallback to AutoDelta\n    pyro.clear_param_store()\n    guide_delta = AutoDelta(model)\n    \n    # Initialize AutoDelta\n    with torch.no_grad():\n        model(sample_x, sample_y)\n        guide_delta(sample_x, sample_y)\n    \n    delta_params = list(guide_delta.parameters())\n    print(f\"AutoDelta parameters: {len(delta_params)}\")\n    \n    if len(delta_params) > 0:\n        optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n        svi = SVI(model, guide_delta, optimizer, loss=Trace_ELBO())\n        \n        print(\"‚úì AutoDelta setup successful!\")\n        return model, guide_delta, svi, \"AutoDelta\"\n    \n    print(\"=== Final fallback: Regular PyTorch training ===\")\n    \n    # Final fallback: regular PyTorch optimizer\n    model_params = list(model.parameters())\n    if len(model_params) > 0:\n        torch_optimizer = torch.optim.Adam(model.parameters(), lr=config.PYRO_LR)\n        print(\"‚úì Regular PyTorch optimizer created!\")\n        print(\"Note: You'll need to use regular PyTorch training loop, not SVI\")\n        return model, None, torch_optimizer, \"PyTorch\"\n    \n    raise ValueError(\"Could not create any optimizer - no parameters found anywhere!\")\n\n# Alternative approach: Force guide parameter creation\ndef force_guide_initialization(model, guide, sample_x, sample_y):\n    \"\"\"Force guide to create parameters by accessing internal methods.\"\"\"\n    \n    # Try to access the guide's internal setup\n    try:\n        # This should force parameter creation\n        guide._setup_prototype(sample_x, sample_y)\n        guide._prototype_trace = guide._get_prototype_trace(sample_x, sample_y)\n        \n        # Now try to get parameters\n        params = list(guide.parameters())\n        print(f\"Forced initialization: {len(params)} parameters\")\n        return len(params) > 0\n    except Exception as e:\n        print(f\"Force initialization failed: {e}\")\n        return False\n\n\n    print(f\"Model parameters: {len(list(model.parameters()))}\")\n    print(\"Model parameter names:\")\n    for name, param in model.named_parameters():\n        print(f\"  {name}: {param.shape}\")\n    \n    # Check if model forward actually has pyro.sample\n    import inspect\n    forward_source = inspect.getsource(model.forward)\n    pyro_samples = forward_source.count('pyro.sample')\n    print(f\"Number of 'pyro.sample' calls in forward: {pyro_samples}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# PYRO MODELS\n# ==============================================================================\n\nclass SimplePyroModel(PyroModule):\n    \"\"\"Simple Bayesian neural network for comparison.\"\"\"\n    \n    def __init__(self, input_dim, output_dim=283):\n        super().__init__()\n        \n        self.network = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](input_dim, config.PYRO_HIDDEN_DIMS[0]),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Dropout](0.1),\n            PyroModule[nn.Linear](config.PYRO_HIDDEN_DIMS[0], config.PYRO_HIDDEN_DIMS[1]),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Dropout](0.1),\n            PyroModule[nn.Linear](config.PYRO_HIDDEN_DIMS[1], output_dim)\n        )\n        \n        # Set simple priors\n        for name, param in self.network.named_parameters():\n            if 'weight' in name:\n                setattr(self.network, name,\n                       PyroSample(dist.Normal(0., 0.5).expand(param.shape).to_event(param.dim())))\n            elif 'bias' in name:\n                setattr(self.network, name,\n                       PyroSample(dist.Normal(0., 0.1).expand(param.shape).to_event(param.dim())))\n\n    def forward(self, x, y=None):\n        mu = self.network(x)\n        \n        # Simple noise model\n        fgs_noise = pyro.sample(\"fgs_noise\", dist.LogNormal(-6., 0.3))\n        airs_noise = pyro.sample(\"airs_noise\", dist.LogNormal(-5., 0.3))\n        \n        noise_tensor = torch.zeros(x.shape[0], mu.shape[1], device=x.device)\n        noise_tensor[:, 0] = fgs_noise\n        noise_tensor[:, 1:] = airs_noise\n        \n        with pyro.plate(\"data\", x.shape[0]):\n            obs = pyro.sample(\"obs\", dist.Normal(mu, noise_tensor).to_event(1), obs=y)\n        \n        return mu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PhysicsInformedPyroModel(PyroModule):\n    \"\"\"Physics-informed model with sophisticated noise modeling.\"\"\"\n    \n    def __init__(self, input_dim, output_dim=283, feature_names=None, wavelengths=None):\n        super().__init__()\n        \n        self.feature_names = feature_names or []\n        self.wavelengths = wavelengths\n        self.output_dim = output_dim\n        \n        # Identify feature types\n        self.stellar_indices = self._find_feature_indices(['Rs', 'Ts', 'Ms', 'log_g'])\n        self.transit_indices = self._find_feature_indices(['fgs_slice', 'fgs_transit', 'fgs_snr'])\n        self.physics_indices = self._find_feature_indices(['stellar_density', 'equilibrium_temp'])\n        \n        # Build networks\n        self.stellar_net = self._build_network(len(self.stellar_indices), 32, 16) if self.stellar_indices else None\n        self.transit_net = self._build_network(len(self.transit_indices), 64, 32) if self.transit_indices else None\n        self.physics_net = self._build_network(len(self.physics_indices), 24, 12) if self.physics_indices else None\n        \n        # Remaining features\n        remaining_dim = input_dim - len(self.stellar_indices + self.transit_indices + self.physics_indices)\n        self.remaining_net = self._build_network(remaining_dim, 48, 24) if remaining_dim > 0 else None\n        \n        # Combiner\n        combiner_input = sum([net.hidden_dim for net in [self.stellar_net, self.transit_net, \n                             self.physics_net, self.remaining_net] if net is not None])\n        \n        self.combiner = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](combiner_input, 128),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Dropout](0.15),\n            PyroModule[nn.Linear](128, 64),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Linear](64, output_dim)\n        )\n        \n        self._set_physics_priors()\n    \n    def _find_feature_indices(self, keywords):\n        \"\"\"Find indices of features containing any of the keywords.\"\"\"\n        indices = []\n        for i, name in enumerate(self.feature_names):\n            if any(keyword.lower() in name.lower() for keyword in keywords):\n                indices.append(i)\n        return indices\n    \n    def _build_network(self, input_dim, hidden_dim, output_dim):\n        \"\"\"Build a simple network with stored hidden dimension.\"\"\"\n        if input_dim == 0:\n            return None\n        \n        net = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](input_dim, hidden_dim),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Linear](hidden_dim, output_dim)\n        )\n        net.hidden_dim = output_dim  # Store for combiner calculation\n        return net\n    \n    def _set_physics_priors(self):\n        \"\"\"Set physics-informed priors.\"\"\"\n        # Stellar: tight priors (well-understood physics)\n        if self.stellar_net:\n            self._set_network_priors(self.stellar_net, weight_scale=0.3, bias_scale=0.05)\n        \n        # Transit: medium priors\n        if self.transit_net:\n            self._set_network_priors(self.transit_net, weight_scale=0.5, bias_scale=0.1)\n        \n        # Physics-derived: tight priors\n        if self.physics_net:\n            self._set_network_priors(self.physics_net, weight_scale=0.4, bias_scale=0.08)\n        \n        # Remaining: wide priors\n        if self.remaining_net:\n            self._set_network_priors(self.remaining_net, weight_scale=0.7, bias_scale=0.15)\n        \n        # Combiner: balanced priors\n        self._set_network_priors(self.combiner, weight_scale=0.5, bias_scale=0.1)\n    \n    def _set_network_priors(self, network, weight_scale, bias_scale):\n        \"\"\"Helper to set priors for a network.\"\"\"\n        for name, param in network.named_parameters():\n            if 'weight' in name:\n                setattr(network, name,\n                       PyroSample(dist.Normal(0., weight_scale).expand(param.shape).to_event(param.dim())))\n            elif 'bias' in name:\n                setattr(network, name,\n                       PyroSample(dist.Normal(0., bias_scale).expand(param.shape).to_event(param.dim())))\n\n    def forward(self, x, y=None):\n        batch_size = x.shape[0]\n        processed_features = []\n        \n        # Process different feature types\n        if self.stellar_net and self.stellar_indices:\n            stellar_processed = self.stellar_net(x[:, self.stellar_indices])\n            processed_features.append(stellar_processed)\n        \n        if self.transit_net and self.transit_indices:\n            transit_processed = self.transit_net(x[:, self.transit_indices])\n            processed_features.append(transit_processed)\n        \n        if self.physics_net and self.physics_indices:\n            physics_processed = self.physics_net(x[:, self.physics_indices])\n            processed_features.append(physics_processed)\n        \n        # Remaining features\n        all_processed = self.stellar_indices + self.transit_indices + self.physics_indices\n        remaining_indices = [i for i in range(x.shape[1]) if i not in all_processed]\n        \n        if self.remaining_net and remaining_indices:\n            remaining_processed = self.remaining_net(x[:, remaining_indices])\n            processed_features.append(remaining_processed)\n        \n        # Combine features\n        if processed_features:\n            combined = torch.cat(processed_features, dim=1)\n        else:\n            combined = x\n        \n        mu = self.combiner(combined)\n        \n        # Sample global noise parameters\n        fgs_base_noise = pyro.sample(\"fgs_base_noise\", \n                                    dist.LogNormal(torch.tensor(-6.0), torch.tensor(0.2)))\n        airs_base_noise = pyro.sample(\"airs_base_noise\", \n                                     dist.LogNormal(torch.tensor(-5.0), torch.tensor(0.2)))\n        \n        # Stellar temperature effects (if available)\n        if self.stellar_indices and len(self.stellar_indices) >= 2:\n            temp_effect = pyro.sample(\"temp_noise_effect\", \n                                     dist.Normal(torch.tensor(0.0), torch.tensor(0.1)))\n            stellar_temp = x[:, self.stellar_indices[1]]  # Assuming 2nd stellar feature is Ts\n            norm_temp = (stellar_temp - 5500) / 1000\n            temp_scaling = torch.exp(temp_effect * norm_temp)\n            \n            # Expand for batch\n            fgs_noise = fgs_base_noise * temp_scaling\n            airs_noise = airs_base_noise * temp_scaling\n        else:\n            # Expand scalars to match batch size\n            fgs_noise = fgs_base_noise.expand(batch_size)\n            airs_noise = airs_base_noise.expand(batch_size)\n        \n        # Wavelength-dependent scaling - make sure it's on correct device\n        with pyro.plate(\"output_wavelengths\", self.output_dim):\n            wavelength_scaling = pyro.sample(\"wavelength_scaling\",\n                                           dist.LogNormal(torch.zeros(self.output_dim, device=x.device), \n                                                        torch.ones(self.output_dim, device=x.device) * 0.1))\n        \n        # Construct noise tensor - ensure proper broadcasting\n        noise_tensor = torch.zeros(batch_size, self.output_dim, device=x.device)\n        \n        # FGS noise (first channel)\n        if isinstance(fgs_noise, torch.Tensor):\n            noise_tensor[:, 0] = fgs_noise * wavelength_scaling[0]\n        else:\n            noise_tensor[:, 0] = fgs_base_noise * wavelength_scaling[0]\n        \n        # AIRS noise (remaining channels)\n        for i in range(1, self.output_dim):\n            if isinstance(airs_noise, torch.Tensor):\n                noise_tensor[:, i] = airs_noise * wavelength_scaling[i]\n            else:\n                noise_tensor[:, i] = airs_base_noise * wavelength_scaling[i]\n        \n        # Ensure positive noise values\n        noise_tensor = torch.clamp(noise_tensor, min=1e-6)\n        \n        # Likelihood with proper plate\n        with pyro.plate(\"data\", batch_size):\n            obs = pyro.sample(\"obs\", \n                             dist.Normal(mu, noise_tensor).to_event(1), \n                             obs=y)\n        \n        return mu\n\ndef train_pyro_model(X_train, y_train, X_val, y_val, feature_names=None, wavelengths=None):\n    \"\"\"Train Pyro model with GPU support.\"\"\"\n    \n    print(f\"\\n--- Training {config.MODEL_TYPE.upper()} model ---\")\n    \n    # Convert to tensors and move to GPU\n    X_train_tensor = torch.FloatTensor(X_train.values if hasattr(X_train, 'values') else X_train).to(device)\n    y_train_tensor = torch.FloatTensor(y_train).to(device)\n    X_val_tensor = torch.FloatTensor(X_val.values if hasattr(X_val, 'values') else X_val).to(device)\n    y_val_tensor = torch.FloatTensor(y_val).to(device)\n    \n    # Clear parameter store\n    pyro.clear_param_store()\n    \n    # Initialize model\n    if config.MODEL_TYPE == 'physics_pyro':\n        model = PhysicsInformedPyroModel(\n            X_train_tensor.shape[1], \n            y_train_tensor.shape[1],\n            feature_names=feature_names,\n            wavelengths=wavelengths\n        ).to(device)\n    else:  # simple_pyro\n        model = SimplePyroModel(\n            X_train_tensor.shape[1],\n            y_train_tensor.shape[1]\n        ).to(device)\n    \n    print(\"Model created successfully\")\n    \n    # Prepare sample data for initialization\n    sample_x = X_train_tensor[:2].to(device)\n    sample_y = y_train_tensor[:2].to(device)\n    \n    # Try different guide approaches\n    guide = None\n    svi = None\n    setup_type = None\n    \n    print(\"=== Trying AutoMultivariateNormal ===\")\n    try:\n        guide = AutoMultivariateNormal(model)\n        \n        # Initialize guide by running both model and guide\n        with torch.no_grad():\n            model(sample_x, sample_y)\n            guide(sample_x, sample_y)\n        \n        guide_params = list(guide.parameters())\n        print(f\"Guide parameters: {len(guide_params)}\")\n        \n        if len(guide_params) > 0:\n            optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n            svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n            \n            # Test SVI step\n            test_loss = svi.step(sample_x, sample_y)\n            print(f\"‚úÖ AutoMultivariateNormal SUCCESS! Test loss: {test_loss:.4f}\")\n            # Watch\n            # with pyro.poutine.trace() as tr:\n            latent = guide.median(sample_x, sample_y)\n            print(\"latent: \", latent)\n            setup_type = \"AutoMultivariateNormal\"\n        else:\n            raise ValueError(\"No guide parameters created\")\n            \n    except Exception as e:\n        print(f\"‚ùå AutoMultivariateNormal failed: {e}\")\n        \n        print(\"=== Trying AutoDelta ===\")\n        try: \n            pyro.clear_param_store()\n            guide = AutoDelta(model)\n            \n            with torch.no_grad():\n                model(sample_x, sample_y)\n                guide(sample_x, sample_y)\n            \n            guide_params = list(guide.parameters())\n            print(f\"AutoDelta parameters: {len(guide_params)}\")\n            \n            if len(guide_params) > 0:\n                optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n                svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n                \n                test_loss = svi.step(sample_x, sample_y)\n                print(f\"‚úÖ AutoDelta SUCCESS! Test loss: {test_loss:.4f}\")\n                setup_type = \"AutoDelta\"\n            else:\n                raise ValueError(\"No AutoDelta parameters created\")\n                \n        except Exception as e2:\n            print(f\"‚ùå AutoDelta failed: {e2}\")\n            \n            print(\"=== Falling back to PyTorch training ===\")\n            # For PyTorch fallback, we need to modify the model's forward method\n            # to not use pyro.sample statements for regular training\n            print(\"‚ùå Pyro guides failed. You need to either:\")\n            print(\"1. Fix the pyro.sample statements in your model\")\n            print(\"2. Use a different training approach\")\n            print(\"3. Check that your model's forward method is compatible with Pyro\")\n            \n            return None, None, None\n    \n    if svi is None:\n        print(\"‚ùå Failed to create SVI - cannot proceed with training\")\n        return None, None, None\n    \n    print(f\"\\nüéâ Setup complete using: {setup_type}\")\n    \n    # Training loop with batching for GPU memory management\n    print(f\"Training for {config.PYRO_EPOCHS} epochs...\")\n    losses = []\n    \n    for epoch in range(config.PYRO_EPOCHS):\n        # Simple batch processing\n        if X_train_tensor.shape[0] > config.PYRO_BATCH_SIZE:\n            # Random batch for stochastic training\n            batch_idx = torch.randperm(X_train_tensor.shape[0])[:config.PYRO_BATCH_SIZE]\n            X_batch = X_train_tensor[batch_idx]\n            y_batch = y_train_tensor[batch_idx]\n        else:\n            X_batch = X_train_tensor\n            y_batch = y_train_tensor\n        \n        loss = svi.step(X_batch, y_batch)\n        losses.append(loss)\n        \n        if epoch % 100 == 0:\n            print(f\"  Epoch {epoch:4d}, Loss: {loss:8.2f}\")\n            if config.USE_GPU:\n                print(f\"  GPU Memory: {torch.cuda.memory_allocated(device) / 1e9:.1f} GB\")\n    \n    # Validation predictions\n    print(\"Generating validation predictions...\")\n    try:\n        predictive = Predictive(model, guide=guide, num_samples=config.PYRO_SAMPLES)\n        \n        with torch.no_grad():\n            model.eval()\n            guide.eval()\n            \n            trace = pyro.poutine.trace(guide).get_trace(X_val_tensor[:1])\n            trace.compute_log_prob()\n            val_predictions = predictive.get_samples(X_val_tensor)\n            print('preds:', val_predictions.keys())  # Should include 'obs' and the latent sites\n            pred_samples = val_predictions['obs']\n            print('pred_samples: ', pred_samples)\n            print(f\"Sample shape: {pred_samples.shape}\")\n            print(f\"Sample stats: min={pred_samples.min()}, max={pred_samples.max()}, mean={pred_samples.mean()}\")\n\n            mean_preds = []\n            for _ in range(config.PYRO_SAMPLES):\n                guide_trace = pyro.poutine.trace(guide).get_trace(X_val_tensor)\n                model_trace = pyro.poutine.trace(pyro.poutine.replay(model, guide_trace)).get_trace(X_val_tensor)\n                mean_preds.append(model_trace.nodes['obs']['value'].cpu())\n        \n            pred_samples_cpu = torch.stack(mean_preds)\n            # Move back to CPU for numpy operations\n            # pred_samples_cpu = pred_samples.cpu()\n            \n            val_quantile_preds = {\n                0.05: torch.quantile(pred_samples_cpu, 0.05, dim=0).numpy(),\n                0.50: torch.quantile(pred_samples_cpu, 0.50, dim=0).numpy(),\n                0.95: torch.quantile(pred_samples_cpu, 0.95, dim=0).numpy()\n            }\n        model.train()\n        guide.train()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Prediction generation failed: {e}\")\n        print(\"Using model mean predictions instead...\")\n        \n        with torch.no_grad():\n            val_mean_pred = model(X_val_tensor).cpu().numpy()\n            val_quantile_preds = {\n                0.05: val_mean_pred * 0.95,  # Simple approximation\n                0.50: val_mean_pred,\n                0.95: val_mean_pred * 1.05\n            }\n    \n    print(f\"Training complete. Final loss: {losses[-1]:.2f}\")\n    return model.cpu(), guide, val_quantile_preds  # Move model back to CPU for saving\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# COMPLETE PYRO PIPELINE: Ariel Data Challenge 2025 \n# Features: Simple/Physics-Informed Pyro models, GPU support, submission handling\n# ==============================================================================\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport functools\nimport os\nfrom tqdm import tqdm\nimport multiprocessing\n\n# Pyro and PyTorch imports\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.nn import PyroModule, PyroSample\nfrom pyro.infer import SVI, Trace_ELBO, Predictive\nfrom pyro.infer.autoguide import AutoMultivariateNormal, AutoDiagonalNormal\nimport torch.nn as nn\n\n# Traditional ML imports (fallback)\nimport scipy.stats\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.multioutput import MultiOutputRegressor\nimport pywt\n\nclass Config:\n    \"\"\"Enhanced configuration with Pyro and GPU support.\"\"\"\n    \n    # Paths\n    DATA_PATH = '/kaggle/input/ariel-data-challenge-2025/'\n    PREPROCESSED_PATH = '/kaggle/input/ariel-data-challenge-2025-af-npy/'\n    OUTPUT_PATH = '/kaggle/working/ariel-data-2025-27'\n    \n    TRAIN_LABELS_PATH = os.path.join(DATA_PATH, 'train.csv')\n    TRAIN_STAR_INFO_PATH = os.path.join(DATA_PATH, 'train_star_info.csv')\n    TEST_STAR_INFO_PATH = os.path.join(DATA_PATH, 'test_star_info.csv')\n    SAMPLE_SUBMISSION_PATH = os.path.join(DATA_PATH, 'sample_submission.csv')\n    WAVELENGTHS_PATH = os.path.join(DATA_PATH, 'wavelengths.csv')\n    A_RAW_PATH = os.path.join(PREPROCESSED_PATH, \"a_raw_train.npy\")\n    F_RAW_PATH = os.path.join(PREPROCESSED_PATH, \"f_raw_train.npy\")\n    \n    # Model selection\n    MODEL_TYPE = 'physics_pyro'  # Options: 'xgboost', 'simple_pyro', 'physics_pyro'\n    \n    # GPU Configuration\n    USE_GPU = False\n    GPU_DEVICE = 'cuda:0'  # Change to 'cuda:1' for second GPU if needed\n    \n    # Pyro hyperparameters\n    PYRO_EPOCHS = 1000\n    PYRO_LR = 0.005\n    PYRO_BATCH_SIZE = 32  # For GPU memory management\n    PYRO_SAMPLES = 200\n    PYRO_HIDDEN_DIMS = [128, 64, 32]\n    \n    # Physics model options\n    USE_WAVELENGTH_PHYSICS = True\n    ADD_DERIVED_PHYSICS_FEATURES = True\n    \n    # Traditional parameters (fallback)\n    VALIDATION_SPLIT = 0.1\n    RANDOM_STATE = 42\n    QUANTILES = [0.05, 0.50, 0.95]\n    XGB_PARAMS = {\n        'n_estimators': 400,\n        'learning_rate': 0.04,\n        'max_depth': 6,\n        'subsample': 0.8,\n        'colsample_bytree': 0.7,\n        'random_state': 42,\n        'tree_method': 'hist',\n    }\n\nconfig = Config()\n\n# GPU Setup\ndef setup_gpu():\n    \"\"\"Setup GPU configuration for PyTorch and Pyro.\"\"\"\n    if config.USE_GPU and torch.cuda.is_available():\n        device = torch.device(config.GPU_DEVICE)\n        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB\")\n        \n        # Optimize for GPU\n        torch.backends.cudnn.benchmark = True\n        torch.cuda.empty_cache()\n        \n        return device\n    else:\n        print(\"Using CPU\")\n        return torch.device('cpu')\n\ndevice = setup_gpu()\n\n# ==============================================================================\n# DATA PREPROCESSING (Your existing functions)\n# ==============================================================================\n\ndef f_read_and_preprocess(dataset, planet_ids):\n    \"\"\"Read the FGS1 files for all planet_ids and extract the time series.\"\"\"\n    print(f\"Preprocessing FGS1 data for {dataset} set...\")\n    f_raw_data = np.full((len(planet_ids), 67500), np.nan, dtype=np.float32)\n    for i, planet_id in tqdm(list(enumerate(planet_ids)), desc=\"FGS1\"):\n        path = f'/kaggle/input/ariel-data-challenge-2025/{dataset}/{int(planet_id)}/FGS1_signal_0.parquet'\n        f_signal = pl.read_parquet(path)\n        mean_signal = f_signal.cast(pl.Int32).sum_horizontal().cast(pl.Float32).to_numpy() / 1024\n        net_signal = mean_signal[1::2] - mean_signal[0::2]\n        f_raw_data[i] = net_signal\n    return f_raw_data\n\ndef a_read_and_preprocess(dataset, planet_ids):\n    \"\"\"Read the AIRS-CH0 files for all planet_ids and extract the time series.\"\"\"\n    print(f\"Preprocessing AIRS-CH0 data for {dataset} set...\")\n    a_raw_data = np.full((len(planet_ids), 5625), np.nan, dtype=np.float32)\n    for i, planet_id in tqdm(list(enumerate(planet_ids)), desc=\"AIRS-CH0\"):\n        path = f'/kaggle/input/ariel-data-challenge-2025/{dataset}/{int(planet_id)}/AIRS-CH0_signal_0.parquet'\n        signal = pl.read_parquet(path)\n        mean_signal = signal.cast(pl.Int32).sum_horizontal().cast(pl.Float32).to_numpy() / (32*356)\n        net_signal = mean_signal[1::2] - mean_signal[0::2]\n        a_raw_data[i] = net_signal\n    return a_raw_data\n\ndef official_competition_score(y_true, y_pred, sigma_pred, naive_mean, naive_sigma,\n                               fsg_sigma_true=1e-6, airs_sigma_true=1e-5, fgs_weight=2.0):\n    \"\"\"Official weighted Gaussian Log-Likelihood metric.\"\"\"\n    y_true, y_pred, sigma_pred = np.array(y_true), np.array(y_pred), np.array(sigma_pred)\n    sigma_pred = np.clip(sigma_pred, 1e-15, None)\n\n    sigma_true_per_channel = np.append(np.array([fsg_sigma_true]), np.ones(y_true.shape[1] - 1) * airs_sigma_true)\n    sigma_true = np.tile(sigma_true_per_channel, (y_true.shape[0], 1))\n\n    GLL_pred = scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred)\n    GLL_true = scipy.stats.norm.logpdf(y_true, loc=y_true, scale=sigma_true)\n    GLL_mean = scipy.stats.norm.logpdf(y_true, loc=naive_mean, scale=naive_sigma)\n    \n    denominator = GLL_true - GLL_mean\n    ind_scores = (GLL_pred - GLL_mean) / (denominator + 1e-9)\n\n    weights_per_channel = np.append(np.array([fgs_weight]), np.ones(y_true.shape[1] - 1))\n    weights = np.tile(weights_per_channel, (y_true.shape[0], 1))\n\n    final_score = np.average(ind_scores, weights=weights)\n    return float(np.clip(final_score, 0.0, 1.0))\n\ndef maximal_feature_engineering(f_raw, a_raw, star_info_df):\n    \"\"\"Your existing feature engineering function.\"\"\"\n    print(\"Engineering features...\")\n    \n    fgs_pre = f_raw[:, :20500]; fgs_post = f_raw[:, 47000:]\n    fgs_unobscured_mean = (fgs_pre.mean(axis=1) + fgs_post.mean(axis=1)) / 2\n    fgs_unobscured_std = (fgs_pre.std(axis=1) + fgs_post.std(axis=1)) / 2\n    fgs_transit = f_raw[:, 23500:44000]\n    \n    features = {}\n    for i in range(5):\n        f_slice_mean = fgs_transit[:, i*4100:(i+1)*4100].mean(axis=1)\n        features[f'fgs_slice_{i+1}'] = (fgs_unobscured_mean - f_slice_mean) / fgs_unobscured_mean\n    \n    features['fgs_transit_std'] = fgs_transit.std(axis=1)\n    features['fgs_transit_skew'] = scipy.stats.skew(fgs_transit, axis=1)\n    features['fgs_transit_kurtosis'] = scipy.stats.kurtosis(fgs_transit, axis=1)\n    features['fgs_snr'] = (fgs_unobscured_mean - fgs_transit.mean(axis=1)) / fgs_unobscured_std\n    features_df = pd.DataFrame(features, index=star_info_df.index)\n\n    fft_coeffs = np.fft.fft(fgs_transit, axis=1)\n    for i in range(1, 6):\n        features_df[f'fgs_fft_mag_{i}'] = np.abs(fft_coeffs[:, i])\n    for level in range(1, 4):\n        coeffs = pywt.wavedec(fgs_transit, 'db4', level=level, axis=1)\n        features_df[f'fgs_wavelet_std_level{level}'] = np.std(coeffs[0], axis=1)\n        features_df[f'fgs_wavelet_mean_level{level}'] = np.mean(coeffs[0], axis=1)\n\n    meta_df = star_info_df.copy().fillna(star_info_df.median())\n    meta_df['log_g_proxy'] = np.log1p(meta_df['Ms']) - 2 * np.log1p(meta_df['Rs'])\n    meta_df['rho_star_proxy'] = meta_df['Ms'] / (meta_df['Rs']**3)\n    \n    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n    poly_cols = ['Rs', 'Ts', 'Mp', 'P']\n    poly_features = poly.fit_transform(meta_df[poly_cols])\n    poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(poly_cols), index=meta_df.index)\n    \n    final_features_df = pd.concat([features_df, meta_df, poly_df], axis=1)\n    final_features_df = final_features_df.loc[:, ~final_features_df.columns.duplicated()]\n\n    if config.ADD_DERIVED_PHYSICS_FEATURES:\n        final_features_df = add_derived_physics_features(final_features_df, meta_df)\n\n    print(f\"Created {final_features_df.shape[1]} features in total.\")\n    return final_features_df.fillna(0)\n\ndef add_derived_physics_features(features_df, star_info_df):\n    \"\"\"Add physics-derived features.\"\"\"\n    enhanced_df = features_df.copy()\n    \n    if 'Rs' in star_info_df.columns and 'Ms' in star_info_df.columns:\n        enhanced_df['stellar_density'] = star_info_df['Ms'] / (star_info_df['Rs'] ** 3)\n        enhanced_df['log_g_calculated'] = np.log10(star_info_df['Ms'] / (star_info_df['Rs'] ** 2))\n    \n    if 'Mp' in star_info_df.columns and 'P' in star_info_df.columns:\n        enhanced_df['semi_major_axis'] = ((star_info_df['P'] / 365.25) ** (2/3) * \n                                        star_info_df['Ms'] ** (1/3))\n        \n        if 'Ts' in star_info_df.columns and 'Rs' in star_info_df.columns:\n            enhanced_df['equilibrium_temp'] = (star_info_df['Ts'] * \n                                             np.sqrt(star_info_df['Rs'] / \n                                                   (2 * enhanced_df['semi_major_axis'])))\n    \n    return enhanced_df\n\ndef train_xgboost_quantiles(X_train, y_train, X_val, y_val):\n    \"\"\"Fallback XGBoost training.\"\"\"\n    print(\"\\\\n--- Training XGBoost Quantile Models ---\")\n    \n    val_quantile_preds = {}\n    for q in config.QUANTILES:\n        print(f\"Training quantile: {q}\")\n        model = xgb.XGBRegressor(**config.XGB_PARAMS, objective='reg:quantileerror', quantile_alpha=q)\n        wrapper = MultiOutputRegressor(model, n_jobs=-1)\n        wrapper.fit(X_train, y_train)\n        val_quantile_preds[q] = wrapper.predict(X_val)\n    \n    return val_quantile_preds\n\n# ==============================================================================\n# SUBMISSION FUNCTIONS\n# ==============================================================================\n\ndef make_pyro_predictions(model, guide, X_test, feature_names=None, wavelengths=None):\n    \"\"\"Make predictions with trained Pyro model.\"\"\"\n    \n    print(\"Making Pyro predictions...\")\n    \n    # Recreate model architecture for loading\n    if config.MODEL_TYPE == 'physics_pyro':\n        model_for_prediction = PhysicsInformedPyroModel(\n            X_test.shape[1], 283,\n            feature_names=feature_names,\n            wavelengths=wavelengths\n        ).to(device)\n    else:\n        model_for_prediction = SimplePyroModel(X_test.shape[1], 283).to(device)\n    \n    # Load state if model is provided as path\n    if isinstance(model, str):\n        model_for_prediction.load_state_dict(torch.load(model, map_location=device))\n        guide.load_state_dict(torch.load(model.replace('model', 'guide'), map_location=device))\n        model = model_for_prediction\n    else:\n        model = model.to(device)\n    \n    X_test_tensor = torch.FloatTensor(X_test.values if hasattr(X_test, 'values') else X_test).to(device)\n    \n    predictive = Predictive(model, guide=guide, num_samples=config.PYRO_SAMPLES)\n    \n    with torch.no_grad():\n        # Process in batches to manage GPU memory\n        all_predictions = []\n        batch_size = min(config.PYRO_BATCH_SIZE, X_test_tensor.shape[0])\n        \n        for i in range(0, X_test_tensor.shape[0], batch_size):\n            X_batch = X_test_tensor[i:i+batch_size]\n            batch_predictions = predictive(X_batch)\n            all_predictions.append(batch_predictions['obs'].cpu())\n        \n        # Combine all predictions\n        pred_samples = torch.cat(all_predictions, dim=1)\n        \n        test_quantile_preds = {\n            0.05: torch.quantile(pred_samples, 0.05, dim=0).numpy(),\n            0.50: torch.quantile(pred_samples, 0.50, dim=0).numpy(),\n            0.95: torch.quantile(pred_samples, 0.95, dim=0).numpy()\n        }\n    \n    return test_quantile_preds\n\ndef save_models(model, guide, feature_names, output_path):\n    \"\"\"Save models and metadata.\"\"\"\n    os.makedirs(output_path, exist_ok=True)\n    \n    if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n        torch.save(model.state_dict(), os.path.join(output_path, 'pyro_model.pth'))\n        torch.save(guide.state_dict(), os.path.join(output_path, 'pyro_guide.pth'))\n        \n        # Save model metadata\n        metadata = {\n            'model_type': config.MODEL_TYPE,\n            'feature_names': feature_names,\n            'input_dim': len(feature_names),\n            'output_dim': 283\n        }\n        \n        with open(os.path.join(output_path, 'model_metadata.pkl'), 'wb') as f:\n            pickle.dump(metadata, f)\n        \n        print(f\"Pyro models saved to: {output_path}\")\n    \n    # Always save feature columns for compatibility\n    with open(os.path.join(output_path, 'feature_columns.pkl'), 'wb') as f:\n        pickle.dump(feature_names, f)\n\ndef load_models(output_path):\n    \"\"\"Load saved models and metadata.\"\"\"\n    \n    # Load metadata\n    with open(os.path.join(output_path, 'model_metadata.pkl'), 'rb') as f:\n        metadata = pickle.load(f)\n    \n    with open(os.path.join(output_path, 'feature_columns.pkl'), 'rb') as f:\n        feature_names = pickle.load(f)\n    \n    # Load appropriate model\n    if metadata['model_type'] == 'physics_pyro':\n        model = PhysicsInformedPyroModel(\n            metadata['input_dim'], \n            metadata['output_dim'],\n            feature_names=metadata['feature_names']\n        )\n    elif metadata['model_type'] == 'simple_pyro':\n        model = SimplePyroModel(\n            metadata['input_dim'],\n            metadata['output_dim']\n        )\n    else:\n        raise ValueError(f\"Unknown model type: {metadata['model_type']}\")\n    \n    # Load model states\n    model.load_state_dict(torch.load(os.path.join(output_path, 'pyro_model.pth'), map_location='cpu'))\n    \n    guide = AutoMultivariateNormal(model)\n    guide.load_state_dict(torch.load(os.path.join(output_path, 'pyro_guide.pth'), map_location='cpu'))\n    \n    return model, guide, feature_names, metadata\n\n# ==============================================================================\n# MAIN EXECUTION PIPELINE\n# ==============================================================================\n\ndef main():\n    \"\"\"Main execution pipeline with model switching.\"\"\"\n    \n    print(f\"\\\\nUsing model: {config.MODEL_TYPE}\")\n    print(f\"Device: {device}\")\n    if config.USE_GPU:\n        print(f\"GPU: {torch.cuda.get_device_name(device)}\")\n    \n    is_submission_run = False # os.path.exists(config.TEST_STAR_INFO_PATH)\n\n    if is_submission_run:\n        # --- SUBMISSION MODE ---\n        print(\"\\\\n\" + \"=\"*60)\n        print(\"======         SUBMISSION MODE          ======\")\n        print(\"=\"*60 + \"\\\\n\")\n\n        # Load test data\n        sample_submission = pd.read_csv(config.SAMPLE_SUBMISSION_PATH, index_col='planet_id')\n        test_star_info_df = pd.read_csv(config.TEST_STAR_INFO_PATH, index_col='planet_id')\n        wavelengths_df = pd.read_csv(config.WAVELENGTHS_PATH)\n        target_column_names = wavelengths_df.columns.tolist()\n\n        # Process test data\n        f_raw_test = f_read_and_preprocess('test', test_star_info_df.index)\n        a_raw_test = a_read_and_preprocess('test', test_star_info_df.index)\n        test_features_df = maximal_feature_engineering(f_raw_test, a_raw_test, test_star_info_df)\n        \n        # Load models and make predictions\n        if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n            try:\n                model, guide, feature_names, metadata = load_models(config.OUTPUT_PATH)\n                test_features_df = test_features_df[feature_names]\n                \n                # Get wavelengths for physics model\n                wavelengths = None\n                if config.MODEL_TYPE == 'physics_pyro' and config.USE_WAVELENGTH_PHYSICS:\n                    try:\n                        # Extract wavelengths from column names\n                        wavelengths = []\n                        for col in target_column_names:\n                            if col.startswith('wl_'):\n                                try:\n                                    wl_val = float(col.replace('wl_', ''))\n                                    wavelengths.append(wl_val)\n                                except:\n                                    wavelengths.append(len(wavelengths) + 1)\n                    except:\n                        wavelengths = None\n                \n                test_quantile_preds = make_pyro_predictions(\n                    model, guide, test_features_df, feature_names, wavelengths\n                )\n                \n            except Exception as e:\n                print(f\"Error loading Pyro model: {e}\")\n                print(\"Falling back to XGBoost...\")\n                config.MODEL_TYPE = 'xgboost'\n        \n        if config.MODEL_TYPE == 'xgboost':\n            # Load XGBoost models (your existing code)\n            with open(os.path.join(config.OUTPUT_PATH, 'feature_columns.pkl'), 'rb') as f:\n                train_cols = pickle.load(f)\n            \n            test_features_df = test_features_df[train_cols]\n            \n            trained_models = {}\n            for q in config.QUANTILES:\n                model_path = os.path.join(config.OUTPUT_PATH, f'model_quantile_{q}.pkl')\n                with open(model_path, 'rb') as f:\n                    trained_models[q] = pickle.load(f)\n            \n            test_quantile_preds = {}\n            for q in config.QUANTILES:\n                test_quantile_preds[q] = trained_models[q].predict(test_features_df)\n\n        # Create submission\n        y_pred_test = test_quantile_preds[0.50].clip(0, None)\n        lower_test, upper_test = test_quantile_preds[0.05], test_quantile_preds[0.95]\n        \n        sigma_raw_test = (upper_test - lower_test) / 3.29\n        sigma_raw_test[sigma_raw_test < 0] = 1e-10\n        \n        # For Pyro models, uncertainties are better calibrated, but we can still apply minimal scaling\n        if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n            sigma_pred_test = sigma_raw_test * 1.1  # Minimal adjustment for Pyro\n        else:\n            # Load calibration for XGBoost\n            try:\n                with open(os.path.join(config.OUTPUT_PATH, 'calibration_params.pkl'), 'rb') as f:\n                    calibration_params = pickle.load(f)\n                sigma_pred_test = (sigma_raw_test * calibration_params['scaling']) + calibration_params['additive']\n            except:\n                sigma_pred_test = sigma_raw_test * 1.2  # Default scaling\n        \n        # Format submission\n        pred_df = pd.DataFrame(y_pred_test, index=sample_submission.index, columns=target_column_names)\n        sigma_df = pd.DataFrame(sigma_pred_test, index=sample_submission.index, \n                               columns=[f\"sigma_{i+1}\" for i in range(len(target_column_names))])\n        submission_df = pd.concat([pred_df, sigma_df], axis=1)\n        \n        submission_df.to_csv('submission.csv')\n        print(\"\\\\n'submission.csv' created successfully!\")\n        print(f\"\\\\nSubmission preview ({config.MODEL_TYPE}):\")\n        print(submission_df.head())\n\n    else:\n        # --- TRAINING MODE ---\n        print(\"\\\\n\" + \"=\"*60)\n        print(\"======          TRAINING MODE           ======\")\n        print(\"=\"*60 + \"\\\\n\")\n        \n        # Load training data\n        train_labels_df = pd.read_csv(config.TRAIN_LABELS_PATH, index_col='planet_id')\n        train_star_info_df = pd.read_csv(config.TRAIN_STAR_INFO_PATH, index_col='planet_id').loc[train_labels_df.index]\n        \n        print(\"Loading pre-processed training data...\")\n        f_raw_train, a_raw_train = np.load(config.F_RAW_PATH), np.load(config.A_RAW_PATH)\n        \n        train_features_df = maximal_feature_engineering(f_raw_train, a_raw_train, train_star_info_df)\n        train_labels = train_labels_df.values\n        naive_mu_train, naive_sigma_train = np.mean(train_labels), np.std(train_labels)\n\n        # Train-validation split\n        X_train, X_val, y_train, y_val = train_test_split(\n            train_features_df, train_labels, \n            test_size=config.VALIDATION_SPLIT, \n            random_state=config.RANDOM_STATE\n        )\n        \n        feature_names = train_features_df.columns.tolist()\n        \n        # Get wavelengths for physics model\n        wavelengths = None\n        if config.MODEL_TYPE == 'physics_pyro' and config.USE_WAVELENGTH_PHYSICS:\n            try:\n                wavelengths_df = pd.read_csv(config.WAVELENGTHS_PATH)\n                wavelengths = []\n                for col in wavelengths_df.columns:\n                    if col.startswith('wl_'):\n                        try:\n                            wl_val = float(col.replace('wl_', ''))\n                            wavelengths.append(wl_val)\n                        except:\n                            wavelengths.append(len(wavelengths) + 1)\n            except:\n                print(\"Could not load wavelengths, using default indexing\")\n                wavelengths = None\n\n        # Train model based on selection\n        if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n            # Train Pyro model\n            model, guide, val_quantile_preds = train_pyro_model(\n                X_train, y_train, X_val, y_val, feature_names, wavelengths\n            )\n            \n            # Evaluate\n            y_pred_val = val_quantile_preds[0.50]\n            lower_val, upper_val = val_quantile_preds[0.05], val_quantile_preds[0.95]\n            sigma_raw_val = (upper_val - lower_val) / 3.29\n            sigma_raw_val[sigma_raw_val < 0] = 1e-10\n            \n            # Minimal calibration for Pyro (they're usually well-calibrated)\n            sigma_val = sigma_raw_val * 1.1\n            \n            pyro_score = official_competition_score(\n                y_val, y_pred_val, sigma_val, naive_mu_train, naive_sigma_train\n            )\n            \n            print(f\"\\\\n{config.MODEL_TYPE.upper()} Validation Score: {pyro_score:.4f}\")\n            \n            # Save models\n            save_models(model, guide, feature_names, config.OUTPUT_PATH)\n            # First model's prediction and evaluation\n            y_pred_val = val_quantile_preds[0.50]\n            lower_val, upper_val = val_quantile_preds[0.05], val_quantile_preds[0.95]\n            sigma_raw_val = (upper_val - lower_val) / 3.29\n            sigma_raw_val[sigma_raw_val < 0] = 1e-10\n            sigma_val = sigma_raw_val * 1.1\n            \n            print(\"DIAGNOSTICS AS PER CHAT 1:\")\n            abs_err = np.abs(y_val - y_pred_val)\n            normed_err = abs_err / sigma_val\n            print(\"Mean absolute error:\", abs_err.mean())\n            print(\"Mean normalized error:\", normed_err.mean())\n            print(\"Max normalized error:\", normed_err.max())\n            print(\"Min normalized error:\", normed_err.min())\n            print(\"END DIAGNOSTICS AS PER CHAT 1\")\n\n            import matplotlib.pyplot as plt\n\n            plt.scatter(y_pred_val, sigma_val, alpha=0.5)\n            plt.xlabel(\"Prediction\")\n            plt.ylabel(\"Predicted œÉ\")\n            plt.title(\"Prediction vs Uncertainty\")\n            plt.grid(True)\n            plt.show()\n\n            \n            pyro_score = official_competition_score(\n                y_val, y_pred_val, sigma_val, naive_mu_train, naive_sigma_train\n            )\n            \n            print(f\"\\nPYRO Validation Score: {pyro_score:.4f}\")\n            print(\"DIAGNOSTICS AS PER CHAT 2:\")\n            print(\"Mean pred:\", y_pred_val.mean().item())\n            print(\"Std pred:\", y_pred_val.std().item())\n            print(\"Any NaN?\", np.isnan(y_pred_val).any())\n            print(\"y_val mean/std:\", y_val.mean().item(), y_val.std().item())\n            print(\"Pred mean/std:\", y_pred_val.mean().item(), y_pred_val.std().item())\n            print(\"Sigma mean:\", sigma_val.mean().item())\n            print(y_pred_val[:5])\n            print(y_val[:5])\n            print(\"END DIAGNOSTICS AS PER CHAT 2\")\n\n            # Retrain on full dataset\n            print(\"\\\\nRetraining on full dataset...\")\n            full_model, full_guide, val_quantile_preds = train_pyro_model(\n                train_features_df, train_labels, \n                train_features_df[:100], train_labels[:100],  # Dummy validation for interface\n                feature_names, wavelengths\n            )\n            \n            # Save final models\n            save_models(full_model, full_guide, feature_names, config.OUTPUT_PATH)\n            \n        else:  # XGBoost\n            val_quantile_preds = train_xgboost_quantiles(X_train, y_train, X_val, y_val)\n            \n            # Your existing calibration logic\n            y_pred_val, lower_val, upper_val = val_quantile_preds[0.50], val_quantile_preds[0.05], val_quantile_preds[0.95]\n            sigma_raw_val = (upper_val - lower_val) / 3.29\n            sigma_raw_val[sigma_raw_val < 0] = 1e-10\n\n            print(\"\\\\nSearching for calibration factors...\")\n            best_score, best_scaling, best_additive = -1.0, 1.0, 0.0\n            \n            for scaling in [1.0, 1.2, 1.5, 2.0, 2.5]:\n                for additive in [0.0, 0.0005, 0.001, 0.0015]:\n                    sigma_calibrated = (sigma_raw_val * scaling) + additive\n                    score = official_competition_score(y_val, y_pred_val, sigma_calibrated, naive_mu_train, naive_sigma_train)\n                    \n                    if score > best_score:\n                        best_score, best_scaling, best_additive = score, scaling, additive\n            \n            print(f\"Best XGBoost Score: {best_score:.4f} (Scale={best_scaling}, Add={best_additive})\")\n            \n            # Save calibration and retrain models\n            os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n            calibration_params = {'scaling': best_scaling, 'additive': best_additive}\n            with open(os.path.join(config.OUTPUT_PATH, 'calibration_params.pkl'), 'wb') as f:\n                pickle.dump(calibration_params, f)\n            \n            with open(os.path.join(config.OUTPUT_PATH, 'feature_columns.pkl'), 'wb') as f:\n                pickle.dump(feature_names, f)\n\n            print(\"\\\\nRetraining XGBoost on full dataset...\")\n            for q in config.QUANTILES:\n                model = xgb.XGBRegressor(**config.XGB_PARAMS, objective='reg:quantileerror', quantile_alpha=q)\n                wrapper = MultiOutputRegressor(model, n_jobs=-1)\n                wrapper.fit(train_features_df, train_labels)\n                \n                model_path = os.path.join(config.OUTPUT_PATH, f'model_quantile_{q}.pkl')\n                with open(model_path, 'wb') as f:\n                    pickle.dump(wrapper, f)\n        \n        print(\"\\\\nTraining complete!\")\n\nif __name__ == '__main__':\n    \n    # Clear GPU memory at start\n    if config.USE_GPU and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    try:\n        main()\n    finally:\n        # Clean up GPU memory\n        if config.USE_GPU and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            print(f\"\\\\nFinal GPU memory usage: {torch.cuda.memory_allocated(device) / 1e9:.1f} GB\")\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":38.472562,"end_time":"2025-06-28T07:57:09.008643","exception":false,"start_time":"2025-06-28T07:56:30.536081","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}