{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101849,"databundleVersionId":13093295,"sourceType":"competition"},{"sourceId":12303949,"sourceType":"datasetVersion","datasetId":7755350},{"sourceId":12309150,"sourceType":"datasetVersion","datasetId":7758622}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":46.011924,"end_time":"2025-06-28T07:57:11.831616","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-28T07:56:25.819692","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install pyro-ppl[extras]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyro\nimport torch\nfrom pyro.infer.autoguide import AutoMultivariateNormal, AutoDelta\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam as PyroAdam\n\ndef setup_pyro_model():\n    \"\"\"Complete setup for Pyro model with proper error handling.\"\"\"\n    \n    # Clear parameter store\n    pyro.clear_param_store()\n    \n    # Create model\n    model = PhysicsInformedPyroModel(\n        X_train_tensor.shape[1], \n        y_train_tensor.shape[1],\n        feature_names=feature_names,\n        wavelengths=wavelengths\n    ).to(device)\n    \n    print(\"=== Attempting AutoMultivariateNormal ===\")\n    \n    # Try AutoMultivariateNormal first\n    guide = AutoMultivariateNormal(model)\n    \n    # Initialize guide properly\n    sample_x = X_train_tensor[:2].to(device)\n    sample_y = y_train_tensor[:2].to(device)\n    \n    # Run model to initialize guide\n    with torch.no_grad():\n        model(sample_x, sample_y)\n    \n    # Force guide initialization by calling it\n    with torch.no_grad():\n        guide(sample_x, sample_y)\n    \n    guide_params = list(guide.parameters())\n    print(f\"AutoMultivariateNormal parameters: {len(guide_params)}\")\n    \n    if len(guide_params) > 0:\n        # Use Pyro optimizer (required for SVI)\n        optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n        svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n        \n        print(\"✓ AutoMultivariateNormal setup successful!\")\n        return model, guide, svi, \"AutoMultivariateNormal\"\n    \n    print(\"=== Fallback to AutoDelta ===\")\n    \n    # Fallback to AutoDelta\n    pyro.clear_param_store()\n    guide_delta = AutoDelta(model)\n    \n    # Initialize AutoDelta\n    with torch.no_grad():\n        model(sample_x, sample_y)\n        guide_delta(sample_x, sample_y)\n    \n    delta_params = list(guide_delta.parameters())\n    print(f\"AutoDelta parameters: {len(delta_params)}\")\n    \n    if len(delta_params) > 0:\n        optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n        svi = SVI(model, guide_delta, optimizer, loss=Trace_ELBO())\n        \n        print(\"✓ AutoDelta setup successful!\")\n        return model, guide_delta, svi, \"AutoDelta\"\n    \n    print(\"=== Final fallback: Regular PyTorch training ===\")\n    \n    # Final fallback: regular PyTorch optimizer\n    model_params = list(model.parameters())\n    if len(model_params) > 0:\n        torch_optimizer = torch.optim.Adam(model.parameters(), lr=config.PYRO_LR)\n        print(\"✓ Regular PyTorch optimizer created!\")\n        print(\"Note: You'll need to use regular PyTorch training loop, not SVI\")\n        return model, None, torch_optimizer, \"PyTorch\"\n    \n    raise ValueError(\"Could not create any optimizer - no parameters found anywhere!\")\n\n# Alternative approach: Force guide parameter creation\ndef force_guide_initialization(model, guide, sample_x, sample_y):\n    \"\"\"Force guide to create parameters by accessing internal methods.\"\"\"\n    \n    # Try to access the guide's internal setup\n    try:\n        # This should force parameter creation\n        guide._setup_prototype(sample_x, sample_y)\n        guide._prototype_trace = guide._get_prototype_trace(sample_x, sample_y)\n        \n        # Now try to get parameters\n        params = list(guide.parameters())\n        print(f\"Forced initialization: {len(params)} parameters\")\n        return len(params) > 0\n    except Exception as e:\n        print(f\"Force initialization failed: {e}\")\n        return False\n\n\n    print(f\"Model parameters: {len(list(model.parameters()))}\")\n    print(\"Model parameter names:\")\n    for name, param in model.named_parameters():\n        print(f\"  {name}: {param.shape}\")\n    \n    # Check if model forward actually has pyro.sample\n    import inspect\n    forward_source = inspect.getsource(model.forward)\n    pyro_samples = forward_source.count('pyro.sample')\n    print(f\"Number of 'pyro.sample' calls in forward: {pyro_samples}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# PYRO MODELS\n# ==============================================================================\n\nclass SimplePyroModel(PyroModule):\n    \"\"\"Simple Bayesian neural network for comparison.\"\"\"\n    \n    def __init__(self, input_dim, output_dim=283):\n        super().__init__()\n        \n        self.network = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](input_dim, config.PYRO_HIDDEN_DIMS[0]),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Dropout](0.1),\n            PyroModule[nn.Linear](config.PYRO_HIDDEN_DIMS[0], config.PYRO_HIDDEN_DIMS[1]),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Dropout](0.1),\n            PyroModule[nn.Linear](config.PYRO_HIDDEN_DIMS[1], output_dim)\n        )\n        \n        # Set simple priors\n        for name, param in self.network.named_parameters():\n            if 'weight' in name:\n                setattr(self.network, name,\n                       PyroSample(dist.Normal(0., 0.5).expand(param.shape).to_event(param.dim())))\n            elif 'bias' in name:\n                setattr(self.network, name,\n                       PyroSample(dist.Normal(0., 0.1).expand(param.shape).to_event(param.dim())))\n\n    def forward(self, x, y=None):\n        mu = self.network(x)\n        \n        # Simple noise model\n        fgs_noise = pyro.sample(\"fgs_noise\", dist.LogNormal(-6., 0.3))\n        airs_noise = pyro.sample(\"airs_noise\", dist.LogNormal(-5., 0.3))\n        \n        noise_tensor = torch.zeros(x.shape[0], mu.shape[1], device=x.device)\n        noise_tensor[:, 0] = fgs_noise\n        noise_tensor[:, 1:] = airs_noise\n        \n        with pyro.plate(\"data\", x.shape[0]):\n            obs = pyro.sample(\"obs\", dist.Normal(mu, noise_tensor).to_event(1), obs=y)\n        \n        return mu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PhysicsInformedPyroModel(PyroModule):\n    \"\"\"Physics-informed model with sophisticated noise modeling.\"\"\"\n    \n    def __init__(self, input_dim, output_dim=283, feature_names=None, wavelengths=None):\n        super().__init__()\n        \n        self.feature_names = feature_names or []\n        self.wavelengths = wavelengths\n        self.output_dim = output_dim\n        \n        # Identify feature types\n        self.stellar_indices = self._find_feature_indices(['Rs', 'Ts', 'Ms', 'log_g'])\n        self.transit_indices = self._find_feature_indices(['fgs_slice', 'fgs_transit', 'fgs_snr'])\n        self.physics_indices = self._find_feature_indices(['stellar_density', 'equilibrium_temp'])\n        \n        # Build networks\n        self.stellar_net = self._build_network(len(self.stellar_indices), 32, 16) if self.stellar_indices else None\n        self.transit_net = self._build_network(len(self.transit_indices), 64, 32) if self.transit_indices else None\n        self.physics_net = self._build_network(len(self.physics_indices), 24, 12) if self.physics_indices else None\n        \n        # Remaining features\n        remaining_dim = input_dim - len(self.stellar_indices + self.transit_indices + self.physics_indices)\n        self.remaining_net = self._build_network(remaining_dim, 48, 24) if remaining_dim > 0 else None\n        \n        # Combiner\n        combiner_input = sum([net.hidden_dim for net in [self.stellar_net, self.transit_net, \n                             self.physics_net, self.remaining_net] if net is not None])\n        \n        self.combiner = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](combiner_input, 128),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Dropout](0.15),\n            PyroModule[nn.Linear](128, 64),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Linear](64, output_dim)\n        )\n        \n        self._set_physics_priors()\n    \n    def _find_feature_indices(self, keywords):\n        \"\"\"Find indices of features containing any of the keywords.\"\"\"\n        indices = []\n        for i, name in enumerate(self.feature_names):\n            if any(keyword.lower() in name.lower() for keyword in keywords):\n                indices.append(i)\n        return indices\n    \n    def _build_network(self, input_dim, hidden_dim, output_dim):\n        \"\"\"Build a simple network with stored hidden dimension.\"\"\"\n        if input_dim == 0:\n            return None\n        \n        net = PyroModule[nn.Sequential](\n            PyroModule[nn.Linear](input_dim, hidden_dim),\n            PyroModule[nn.ReLU](),\n            PyroModule[nn.Linear](hidden_dim, output_dim)\n        )\n        net.hidden_dim = output_dim  # Store for combiner calculation\n        return net\n    \n    def _set_physics_priors(self):\n        \"\"\"Set physics-informed priors.\"\"\"\n        # Stellar: tight priors (well-understood physics)\n        if self.stellar_net:\n            self._set_network_priors(self.stellar_net, weight_scale=0.3, bias_scale=0.05)\n        \n        # Transit: medium priors\n        if self.transit_net:\n            self._set_network_priors(self.transit_net, weight_scale=0.5, bias_scale=0.1)\n        \n        # Physics-derived: tight priors\n        if self.physics_net:\n            self._set_network_priors(self.physics_net, weight_scale=0.4, bias_scale=0.08)\n        \n        # Remaining: wide priors\n        if self.remaining_net:\n            self._set_network_priors(self.remaining_net, weight_scale=0.7, bias_scale=0.15)\n        \n        # Combiner: balanced priors\n        self._set_network_priors(self.combiner, weight_scale=0.5, bias_scale=0.1)\n    \n    def _set_network_priors(self, network, weight_scale, bias_scale):\n        \"\"\"Helper to set priors for a network.\"\"\"\n        for name, param in network.named_parameters():\n            if 'weight' in name:\n                setattr(network, name,\n                       PyroSample(dist.Normal(0., weight_scale).expand(param.shape).to_event(param.dim())))\n            elif 'bias' in name:\n                setattr(network, name,\n                       PyroSample(dist.Normal(0., bias_scale).expand(param.shape).to_event(param.dim())))\n\n    def forward(self, x, y=None):\n        batch_size = x.shape[0]\n        processed_features = []\n        \n        # Process different feature types\n        if self.stellar_net and self.stellar_indices:\n            stellar_processed = self.stellar_net(x[:, self.stellar_indices])\n            processed_features.append(stellar_processed)\n        \n        if self.transit_net and self.transit_indices:\n            transit_processed = self.transit_net(x[:, self.transit_indices])\n            processed_features.append(transit_processed)\n        \n        if self.physics_net and self.physics_indices:\n            physics_processed = self.physics_net(x[:, self.physics_indices])\n            processed_features.append(physics_processed)\n        \n        # Remaining features\n        all_processed = self.stellar_indices + self.transit_indices + self.physics_indices\n        remaining_indices = [i for i in range(x.shape[1]) if i not in all_processed]\n        \n        if self.remaining_net and remaining_indices:\n            remaining_processed = self.remaining_net(x[:, remaining_indices])\n            processed_features.append(remaining_processed)\n        \n        # Combine features\n        if processed_features:\n            combined = torch.cat(processed_features, dim=1)\n        else:\n            combined = x\n        \n        mu = self.combiner(combined)\n        \n        # Sample global noise parameters\n        fgs_base_noise = pyro.sample(\"fgs_base_noise\", \n                                    dist.LogNormal(torch.tensor(-6.0), torch.tensor(0.2)))\n        airs_base_noise = pyro.sample(\"airs_base_noise\", \n                                     dist.LogNormal(torch.tensor(-5.0), torch.tensor(0.2)))\n        \n        # Stellar temperature effects (if available)\n        if self.stellar_indices and len(self.stellar_indices) >= 2:\n            temp_effect = pyro.sample(\"temp_noise_effect\", \n                                     dist.Normal(torch.tensor(0.0), torch.tensor(0.1)))\n            stellar_temp = x[:, self.stellar_indices[1]]  # Assuming 2nd stellar feature is Ts\n            norm_temp = (stellar_temp - 5500) / 1000\n            temp_scaling = torch.exp(temp_effect * norm_temp)\n            \n            # Expand for batch\n            fgs_noise = fgs_base_noise * temp_scaling\n            airs_noise = airs_base_noise * temp_scaling\n        else:\n            # Expand scalars to match batch size\n            fgs_noise = fgs_base_noise.expand(batch_size)\n            airs_noise = airs_base_noise.expand(batch_size)\n        \n        # Wavelength-dependent scaling - make sure it's on correct device\n        with pyro.plate(\"output_wavelengths\", self.output_dim):\n            wavelength_scaling = pyro.sample(\"wavelength_scaling\",\n                                           dist.LogNormal(torch.zeros(self.output_dim, device=x.device), \n                                                        torch.ones(self.output_dim, device=x.device) * 0.1))\n        \n        # Construct noise tensor - ensure proper broadcasting\n        noise_tensor = torch.zeros(batch_size, self.output_dim, device=x.device)\n        \n        # FGS noise (first channel)\n        if isinstance(fgs_noise, torch.Tensor):\n            noise_tensor[:, 0] = fgs_noise * wavelength_scaling[0]\n        else:\n            noise_tensor[:, 0] = fgs_base_noise * wavelength_scaling[0]\n        \n        # AIRS noise (remaining channels)\n        for i in range(1, self.output_dim):\n            if isinstance(airs_noise, torch.Tensor):\n                noise_tensor[:, i] = airs_noise * wavelength_scaling[i]\n            else:\n                noise_tensor[:, i] = airs_base_noise * wavelength_scaling[i]\n        \n        # Ensure positive noise values\n        noise_tensor = torch.clamp(noise_tensor, min=1e-6)\n        \n        # Likelihood with proper plate\n        with pyro.plate(\"data\", batch_size):\n            obs = pyro.sample(\"obs\", \n                             dist.Normal(mu, noise_tensor).to_event(1), \n                             obs=y)\n        \n        return mu\n\nimport pyro\nimport torch\nfrom pyro.infer.autoguide import AutoMultivariateNormal, AutoDelta\nfrom pyro.infer import SVI, Trace_ELBO, Predictive\nfrom pyro.optim import Adam as PyroAdam\n\ndef train_pyro_model(X_train, y_train, X_val, y_val, feature_names=None, wavelengths=None):\n    \"\"\"Train Pyro model with GPU support.\"\"\"\n    \n    print(f\"\\n--- Training {config.MODEL_TYPE.upper()} model ---\")\n    \n    # Convert to tensors and move to GPU\n    X_train_tensor = torch.FloatTensor(X_train.values if hasattr(X_train, 'values') else X_train).to(device)\n    y_train_tensor = torch.FloatTensor(y_train).to(device)\n    X_val_tensor = torch.FloatTensor(X_val.values if hasattr(X_val, 'values') else X_val).to(device)\n    y_val_tensor = torch.FloatTensor(y_val).to(device)\n    \n    # Clear parameter store\n    pyro.clear_param_store()\n    \n    # Initialize model\n    if config.MODEL_TYPE == 'physics_pyro':\n        model = PhysicsInformedPyroModel(\n            X_train_tensor.shape[1], \n            y_train_tensor.shape[1],\n            feature_names=feature_names,\n            wavelengths=wavelengths\n        ).to(device)\n    else:  # simple_pyro\n        model = SimplePyroModel(\n            X_train_tensor.shape[1],\n            y_train_tensor.shape[1]\n        ).to(device)\n    \n    print(\"Model created successfully\")\n    \n    # Prepare sample data for initialization\n    sample_x = X_train_tensor[:2].to(device)\n    sample_y = y_train_tensor[:2].to(device)\n    \n    # Try different guide approaches\n    guide = None\n    svi = None\n    setup_type = None\n    \n    print(\"=== Trying AutoMultivariateNormal ===\")\n    try:\n        guide = AutoMultivariateNormal(model)\n        \n        # Initialize guide by running both model and guide\n        with torch.no_grad():\n            model(sample_x, sample_y)\n            guide(sample_x, sample_y)\n        \n        guide_params = list(guide.parameters())\n        print(f\"Guide parameters: {len(guide_params)}\")\n        \n        if len(guide_params) > 0:\n            optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n            svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n            \n            # Test SVI step\n            test_loss = svi.step(sample_x, sample_y)\n            print(f\"✅ AutoMultivariateNormal SUCCESS! Test loss: {test_loss:.4f}\")\n            setup_type = \"AutoMultivariateNormal\"\n        else:\n            raise ValueError(\"No guide parameters created\")\n            \n    except Exception as e:\n        print(f\"❌ AutoMultivariateNormal failed: {e}\")\n        \n        print(\"=== Trying AutoDelta ===\")\n        try:\n            pyro.clear_param_store()\n            guide = AutoDelta(model)\n            \n            with torch.no_grad():\n                model(sample_x, sample_y)\n                guide(sample_x, sample_y)\n            \n            guide_params = list(guide.parameters())\n            print(f\"AutoDelta parameters: {len(guide_params)}\")\n            \n            if len(guide_params) > 0:\n                optimizer = PyroAdam({\"lr\": config.PYRO_LR})\n                svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n                \n                test_loss = svi.step(sample_x, sample_y)\n                print(f\"✅ AutoDelta SUCCESS! Test loss: {test_loss:.4f}\")\n                setup_type = \"AutoDelta\"\n            else:\n                raise ValueError(\"No AutoDelta parameters created\")\n                \n        except Exception as e2:\n            print(f\"❌ AutoDelta failed: {e2}\")\n            \n            print(\"=== Falling back to PyTorch training ===\")\n            # For PyTorch fallback, we need to modify the model's forward method\n            # to not use pyro.sample statements for regular training\n            print(\"❌ Pyro guides failed. You need to either:\")\n            print(\"1. Fix the pyro.sample statements in your model\")\n            print(\"2. Use a different training approach\")\n            print(\"3. Check that your model's forward method is compatible with Pyro\")\n            \n            return None, None, None\n    \n    if svi is None:\n        print(\"❌ Failed to create SVI - cannot proceed with training\")\n        return None, None, None\n    \n    print(f\"\\n🎉 Setup complete using: {setup_type}\")\n    \n    # Training loop with batching for GPU memory management\n    print(f\"Training for {config.PYRO_EPOCHS} epochs...\")\n    losses = []\n    \n    for epoch in range(config.PYRO_EPOCHS):\n        # Simple batch processing\n        if X_train_tensor.shape[0] > config.PYRO_BATCH_SIZE:\n            # Random batch for stochastic training\n            batch_idx = torch.randperm(X_train_tensor.shape[0])[:config.PYRO_BATCH_SIZE]\n            X_batch = X_train_tensor[batch_idx]\n            y_batch = y_train_tensor[batch_idx]\n        else:\n            X_batch = X_train_tensor\n            y_batch = y_train_tensor\n        \n        loss = svi.step(X_batch, y_batch)\n        losses.append(loss)\n        \n        if epoch % 100 == 0:\n            print(f\"  Epoch {epoch:4d}, Loss: {loss:8.2f}\")\n            if config.USE_GPU:\n                print(f\"  GPU Memory: {torch.cuda.memory_allocated(device) / 1e9:.1f} GB\")\n    \n    # Validation predictions\n    print(\"Generating validation predictions...\")\n    try:\n        predictive = Predictive(model, guide=guide, num_samples=config.PYRO_SAMPLES)\n        \n        with torch.no_grad():\n            val_predictions = predictive(X_val_tensor)\n            pred_samples = val_predictions['obs']\n            \n            # Move back to CPU for numpy operations\n            pred_samples_cpu = pred_samples.cpu()\n            \n            val_quantile_preds = {\n                0.05: torch.quantile(pred_samples_cpu, 0.05, dim=0).numpy(),\n                0.50: torch.quantile(pred_samples_cpu, 0.50, dim=0).numpy(),\n                0.95: torch.quantile(pred_samples_cpu, 0.95, dim=0).numpy()\n            }\n            \n    except Exception as e:\n        print(f\"⚠️ Prediction generation failed: {e}\")\n        print(\"Using model mean predictions instead...\")\n        \n        with torch.no_grad():\n            val_mean_pred = model(X_val_tensor).cpu().numpy()\n            val_quantile_preds = {\n                0.05: val_mean_pred * 0.95,  # Simple approximation\n                0.50: val_mean_pred,\n                0.95: val_mean_pred * 1.05\n            }\n    \n    print(f\"Training complete. Final loss: {losses[-1]:.2f}\")\n    return model.cpu(), guide, val_quantile_preds  # Move model back to CPU for saving\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T14:54:56.145106Z","iopub.execute_input":"2025-08-05T14:54:56.146500Z","iopub.status.idle":"2025-08-05T14:54:56.192478Z","shell.execute_reply.started":"2025-08-05T14:54:56.146457Z","shell.execute_reply":"2025-08-05T14:54:56.191295Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# ==============================================================================\n# COMPLETE PYRO PIPELINE: Ariel Data Challenge 2025 \n# Features: Simple/Physics-Informed Pyro models, GPU support, submission handling\n# ==============================================================================\n\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport functools\nimport os\nfrom tqdm import tqdm\nimport multiprocessing\n\n# Pyro and PyTorch imports\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.nn import PyroModule, PyroSample\nfrom pyro.infer import SVI, Trace_ELBO, Predictive\nfrom pyro.infer.autoguide import AutoMultivariateNormal, AutoDiagonalNormal\nimport torch.nn as nn\n\n# Traditional ML imports (fallback)\nimport scipy.stats\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.multioutput import MultiOutputRegressor\nimport pywt\n\nclass Config:\n    \"\"\"Enhanced configuration with Pyro and GPU support.\"\"\"\n    \n    # Paths\n    DATA_PATH = '/kaggle/input/ariel-data-challenge-2025/'\n    PREPROCESSED_PATH = '/kaggle/input/ariel-data-challenge-2025-af-npy/'\n    OUTPUT_PATH = '/kaggle/working/ariel-data-2025-27'\n    \n    TRAIN_LABELS_PATH = os.path.join(DATA_PATH, 'train.csv')\n    TRAIN_STAR_INFO_PATH = os.path.join(DATA_PATH, 'train_star_info.csv')\n    TEST_STAR_INFO_PATH = os.path.join(DATA_PATH, 'test_star_info.csv')\n    SAMPLE_SUBMISSION_PATH = os.path.join(DATA_PATH, 'sample_submission.csv')\n    WAVELENGTHS_PATH = os.path.join(DATA_PATH, 'wavelengths.csv')\n    A_RAW_PATH = os.path.join(PREPROCESSED_PATH, \"a_raw_train.npy\")\n    F_RAW_PATH = os.path.join(PREPROCESSED_PATH, \"f_raw_train.npy\")\n    \n    # Model selection\n    MODEL_TYPE = 'physics_pyro'  # Options: 'xgboost', 'simple_pyro', 'physics_pyro'\n    \n    # GPU Configuration\n    USE_GPU = False\n    GPU_DEVICE = 'cuda:0'  # Change to 'cuda:1' for second GPU if needed\n    \n    # Pyro hyperparameters\n    PYRO_EPOCHS = 100000\n    PYRO_LR = 0.000005\n    PYRO_BATCH_SIZE = 32  # For GPU memory management\n    PYRO_SAMPLES = 200\n    PYRO_HIDDEN_DIMS = [128, 64, 32]\n    \n    # Physics model options\n    USE_WAVELENGTH_PHYSICS = True\n    ADD_DERIVED_PHYSICS_FEATURES = True\n    \n    # Traditional parameters (fallback)\n    VALIDATION_SPLIT = 0.1\n    RANDOM_STATE = 42\n    QUANTILES = [0.05, 0.50, 0.95]\n    XGB_PARAMS = {\n        'n_estimators': 400,\n        'learning_rate': 0.04,\n        'max_depth': 6,\n        'subsample': 0.8,\n        'colsample_bytree': 0.7,\n        'random_state': 42,\n        'tree_method': 'hist',\n    }\n\nconfig = Config()\n\n# GPU Setup\ndef setup_gpu():\n    \"\"\"Setup GPU configuration for PyTorch and Pyro.\"\"\"\n    if config.USE_GPU and torch.cuda.is_available():\n        device = torch.device(config.GPU_DEVICE)\n        print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB\")\n        \n        # Optimize for GPU\n        torch.backends.cudnn.benchmark = True\n        torch.cuda.empty_cache()\n        \n        return device\n    else:\n        print(\"Using CPU\")\n        return torch.device('cpu')\n\ndevice = setup_gpu()\n\n# ==============================================================================\n# DATA PREPROCESSING (Your existing functions)\n# ==============================================================================\n\ndef f_read_and_preprocess(dataset, planet_ids):\n    \"\"\"Read the FGS1 files for all planet_ids and extract the time series.\"\"\"\n    print(f\"Preprocessing FGS1 data for {dataset} set...\")\n    f_raw_data = np.full((len(planet_ids), 67500), np.nan, dtype=np.float32)\n    for i, planet_id in tqdm(list(enumerate(planet_ids)), desc=\"FGS1\"):\n        path = f'/kaggle/input/ariel-data-challenge-2025/{dataset}/{int(planet_id)}/FGS1_signal_0.parquet'\n        f_signal = pl.read_parquet(path)\n        mean_signal = f_signal.cast(pl.Int32).sum_horizontal().cast(pl.Float32).to_numpy() / 1024\n        net_signal = mean_signal[1::2] - mean_signal[0::2]\n        f_raw_data[i] = net_signal\n    return f_raw_data\n\ndef a_read_and_preprocess(dataset, planet_ids):\n    \"\"\"Read the AIRS-CH0 files for all planet_ids and extract the time series.\"\"\"\n    print(f\"Preprocessing AIRS-CH0 data for {dataset} set...\")\n    a_raw_data = np.full((len(planet_ids), 5625), np.nan, dtype=np.float32)\n    for i, planet_id in tqdm(list(enumerate(planet_ids)), desc=\"AIRS-CH0\"):\n        path = f'/kaggle/input/ariel-data-challenge-2025/{dataset}/{int(planet_id)}/AIRS-CH0_signal_0.parquet'\n        signal = pl.read_parquet(path)\n        mean_signal = signal.cast(pl.Int32).sum_horizontal().cast(pl.Float32).to_numpy() / (32*356)\n        net_signal = mean_signal[1::2] - mean_signal[0::2]\n        a_raw_data[i] = net_signal\n    return a_raw_data\n\ndef official_competition_score(y_true, y_pred, sigma_pred, naive_mean, naive_sigma,\n                               fsg_sigma_true=1e-6, airs_sigma_true=1e-5, fgs_weight=2.0):\n    \"\"\"Official weighted Gaussian Log-Likelihood metric.\"\"\"\n    y_true, y_pred, sigma_pred = np.array(y_true), np.array(y_pred), np.array(sigma_pred)\n    sigma_pred = np.clip(sigma_pred, 1e-15, None)\n\n    sigma_true_per_channel = np.append(np.array([fsg_sigma_true]), np.ones(y_true.shape[1] - 1) * airs_sigma_true)\n    sigma_true = np.tile(sigma_true_per_channel, (y_true.shape[0], 1))\n\n    GLL_pred = scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred)\n    GLL_true = scipy.stats.norm.logpdf(y_true, loc=y_true, scale=sigma_true)\n    GLL_mean = scipy.stats.norm.logpdf(y_true, loc=naive_mean, scale=naive_sigma)\n    \n    denominator = GLL_true - GLL_mean\n    ind_scores = (GLL_pred - GLL_mean) / (denominator + 1e-9)\n\n    weights_per_channel = np.append(np.array([fgs_weight]), np.ones(y_true.shape[1] - 1))\n    weights = np.tile(weights_per_channel, (y_true.shape[0], 1))\n\n    final_score = np.average(ind_scores, weights=weights)\n    return float(np.clip(final_score, 0.0, 1.0))\n\ndef maximal_feature_engineering(f_raw, a_raw, star_info_df):\n    \"\"\"Your existing feature engineering function.\"\"\"\n    print(\"Engineering features...\")\n    \n    fgs_pre = f_raw[:, :20500]; fgs_post = f_raw[:, 47000:]\n    fgs_unobscured_mean = (fgs_pre.mean(axis=1) + fgs_post.mean(axis=1)) / 2\n    fgs_unobscured_std = (fgs_pre.std(axis=1) + fgs_post.std(axis=1)) / 2\n    fgs_transit = f_raw[:, 23500:44000]\n    \n    features = {}\n    for i in range(5):\n        f_slice_mean = fgs_transit[:, i*4100:(i+1)*4100].mean(axis=1)\n        features[f'fgs_slice_{i+1}'] = (fgs_unobscured_mean - f_slice_mean) / fgs_unobscured_mean\n    \n    features['fgs_transit_std'] = fgs_transit.std(axis=1)\n    features['fgs_transit_skew'] = scipy.stats.skew(fgs_transit, axis=1)\n    features['fgs_transit_kurtosis'] = scipy.stats.kurtosis(fgs_transit, axis=1)\n    features['fgs_snr'] = (fgs_unobscured_mean - fgs_transit.mean(axis=1)) / fgs_unobscured_std\n    features_df = pd.DataFrame(features, index=star_info_df.index)\n\n    fft_coeffs = np.fft.fft(fgs_transit, axis=1)\n    for i in range(1, 6):\n        features_df[f'fgs_fft_mag_{i}'] = np.abs(fft_coeffs[:, i])\n    for level in range(1, 4):\n        coeffs = pywt.wavedec(fgs_transit, 'db4', level=level, axis=1)\n        features_df[f'fgs_wavelet_std_level{level}'] = np.std(coeffs[0], axis=1)\n        features_df[f'fgs_wavelet_mean_level{level}'] = np.mean(coeffs[0], axis=1)\n\n    meta_df = star_info_df.copy().fillna(star_info_df.median())\n    meta_df['log_g_proxy'] = np.log1p(meta_df['Ms']) - 2 * np.log1p(meta_df['Rs'])\n    meta_df['rho_star_proxy'] = meta_df['Ms'] / (meta_df['Rs']**3)\n    \n    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n    poly_cols = ['Rs', 'Ts', 'Mp', 'P']\n    poly_features = poly.fit_transform(meta_df[poly_cols])\n    poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(poly_cols), index=meta_df.index)\n    \n    final_features_df = pd.concat([features_df, meta_df, poly_df], axis=1)\n    final_features_df = final_features_df.loc[:, ~final_features_df.columns.duplicated()]\n\n    if config.ADD_DERIVED_PHYSICS_FEATURES:\n        final_features_df = add_derived_physics_features(final_features_df, meta_df)\n\n    print(f\"Created {final_features_df.shape[1]} features in total.\")\n    return final_features_df.fillna(0)\n\ndef add_derived_physics_features(features_df, star_info_df):\n    \"\"\"Add physics-derived features.\"\"\"\n    enhanced_df = features_df.copy()\n    \n    if 'Rs' in star_info_df.columns and 'Ms' in star_info_df.columns:\n        enhanced_df['stellar_density'] = star_info_df['Ms'] / (star_info_df['Rs'] ** 3)\n        enhanced_df['log_g_calculated'] = np.log10(star_info_df['Ms'] / (star_info_df['Rs'] ** 2))\n    \n    if 'Mp' in star_info_df.columns and 'P' in star_info_df.columns:\n        enhanced_df['semi_major_axis'] = ((star_info_df['P'] / 365.25) ** (2/3) * \n                                        star_info_df['Ms'] ** (1/3))\n        \n        if 'Ts' in star_info_df.columns and 'Rs' in star_info_df.columns:\n            enhanced_df['equilibrium_temp'] = (star_info_df['Ts'] * \n                                             np.sqrt(star_info_df['Rs'] / \n                                                   (2 * enhanced_df['semi_major_axis'])))\n    \n    return enhanced_df\n\ndef train_xgboost_quantiles(X_train, y_train, X_val, y_val):\n    \"\"\"Fallback XGBoost training.\"\"\"\n    print(\"\\\\n--- Training XGBoost Quantile Models ---\")\n    \n    val_quantile_preds = {}\n    for q in config.QUANTILES:\n        print(f\"Training quantile: {q}\")\n        model = xgb.XGBRegressor(**config.XGB_PARAMS, objective='reg:quantileerror', quantile_alpha=q)\n        wrapper = MultiOutputRegressor(model, n_jobs=-1)\n        wrapper.fit(X_train, y_train)\n        val_quantile_preds[q] = wrapper.predict(X_val)\n    \n    return val_quantile_preds\n\n# ==============================================================================\n# SUBMISSION FUNCTIONS\n# ==============================================================================\n\ndef make_pyro_predictions(model, guide, X_test, feature_names=None, wavelengths=None):\n    \"\"\"Make predictions with trained Pyro model.\"\"\"\n    \n    print(\"Making Pyro predictions...\")\n    \n    # Recreate model architecture for loading\n    if config.MODEL_TYPE == 'physics_pyro':\n        model_for_prediction = PhysicsInformedPyroModel(\n            X_test.shape[1], 283,\n            feature_names=feature_names,\n            wavelengths=wavelengths\n        ).to(device)\n    else:\n        model_for_prediction = SimplePyroModel(X_test.shape[1], 283).to(device)\n    \n    # Load state if model is provided as path\n    if isinstance(model, str):\n        model_for_prediction.load_state_dict(torch.load(model, map_location=device))\n        guide.load_state_dict(torch.load(model.replace('model', 'guide'), map_location=device))\n        model = model_for_prediction\n    else:\n        model = model.to(device)\n    \n    X_test_tensor = torch.FloatTensor(X_test.values if hasattr(X_test, 'values') else X_test).to(device)\n    \n    predictive = Predictive(model, guide=guide, num_samples=config.PYRO_SAMPLES)\n    \n    with torch.no_grad():\n        # Process in batches to manage GPU memory\n        all_predictions = []\n        batch_size = min(config.PYRO_BATCH_SIZE, X_test_tensor.shape[0])\n        \n        for i in range(0, X_test_tensor.shape[0], batch_size):\n            X_batch = X_test_tensor[i:i+batch_size]\n            batch_predictions = predictive(X_batch)\n            all_predictions.append(batch_predictions['obs'].cpu())\n        \n        # Combine all predictions\n        pred_samples = torch.cat(all_predictions, dim=1)\n        \n        test_quantile_preds = {\n            0.05: torch.quantile(pred_samples, 0.05, dim=0).numpy(),\n            0.50: torch.quantile(pred_samples, 0.50, dim=0).numpy(),\n            0.95: torch.quantile(pred_samples, 0.95, dim=0).numpy()\n        }\n    \n    return test_quantile_preds\n\ndef save_models(model, guide, feature_names, output_path):\n    \"\"\"Save models and metadata.\"\"\"\n    os.makedirs(output_path, exist_ok=True)\n    \n    if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n        torch.save(model.state_dict(), os.path.join(output_path, 'pyro_model.pth'))\n        torch.save(guide.state_dict(), os.path.join(output_path, 'pyro_guide.pth'))\n        \n        # Save model metadata\n        metadata = {\n            'model_type': config.MODEL_TYPE,\n            'feature_names': feature_names,\n            'input_dim': len(feature_names),\n            'output_dim': 283\n        }\n        \n        with open(os.path.join(output_path, 'model_metadata.pkl'), 'wb') as f:\n            pickle.dump(metadata, f)\n        \n        print(f\"Pyro models saved to: {output_path}\")\n    \n    # Always save feature columns for compatibility\n    with open(os.path.join(output_path, 'feature_columns.pkl'), 'wb') as f:\n        pickle.dump(feature_names, f)\n\ndef load_models(output_path):\n    \"\"\"Load saved models and metadata.\"\"\"\n    \n    # Load metadata\n    with open(os.path.join(output_path, 'model_metadata.pkl'), 'rb') as f:\n        metadata = pickle.load(f)\n    \n    with open(os.path.join(output_path, 'feature_columns.pkl'), 'rb') as f:\n        feature_names = pickle.load(f)\n    \n    # Load appropriate model\n    if metadata['model_type'] == 'physics_pyro':\n        model = PhysicsInformedPyroModel(\n            metadata['input_dim'], \n            metadata['output_dim'],\n            feature_names=metadata['feature_names']\n        )\n    elif metadata['model_type'] == 'simple_pyro':\n        model = SimplePyroModel(\n            metadata['input_dim'],\n            metadata['output_dim']\n        )\n    else:\n        raise ValueError(f\"Unknown model type: {metadata['model_type']}\")\n    \n    # Load model states\n    model.load_state_dict(torch.load(os.path.join(output_path, 'pyro_model.pth'), map_location='cpu'))\n    \n    guide = AutoMultivariateNormal(model)\n    guide.load_state_dict(torch.load(os.path.join(output_path, 'pyro_guide.pth'), map_location='cpu'))\n    \n    return model, guide, feature_names, metadata\n\n# ==============================================================================\n# MAIN EXECUTION PIPELINE\n# ==============================================================================\n\ndef main():\n    \"\"\"Main execution pipeline with model switching.\"\"\"\n    \n    print(f\"\\\\nUsing model: {config.MODEL_TYPE}\")\n    print(f\"Device: {device}\")\n    if config.USE_GPU:\n        print(f\"GPU: {torch.cuda.get_device_name(device)}\")\n    \n    is_submission_run = False # os.path.exists(config.TEST_STAR_INFO_PATH)\n\n    if is_submission_run:\n        # --- SUBMISSION MODE ---\n        print(\"\\\\n\" + \"=\"*60)\n        print(\"======         SUBMISSION MODE          ======\")\n        print(\"=\"*60 + \"\\\\n\")\n\n        # Load test data\n        sample_submission = pd.read_csv(config.SAMPLE_SUBMISSION_PATH, index_col='planet_id')\n        test_star_info_df = pd.read_csv(config.TEST_STAR_INFO_PATH, index_col='planet_id')\n        wavelengths_df = pd.read_csv(config.WAVELENGTHS_PATH)\n        target_column_names = wavelengths_df.columns.tolist()\n\n        # Process test data\n        f_raw_test = f_read_and_preprocess('test', test_star_info_df.index)\n        a_raw_test = a_read_and_preprocess('test', test_star_info_df.index)\n        test_features_df = maximal_feature_engineering(f_raw_test, a_raw_test, test_star_info_df)\n        \n        # Load models and make predictions\n        if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n            try:\n                model, guide, feature_names, metadata = load_models(config.OUTPUT_PATH)\n                test_features_df = test_features_df[feature_names]\n                \n                # Get wavelengths for physics model\n                wavelengths = None\n                if config.MODEL_TYPE == 'physics_pyro' and config.USE_WAVELENGTH_PHYSICS:\n                    try:\n                        # Extract wavelengths from column names\n                        wavelengths = []\n                        for col in target_column_names:\n                            if col.startswith('wl_'):\n                                try:\n                                    wl_val = float(col.replace('wl_', ''))\n                                    wavelengths.append(wl_val)\n                                except:\n                                    wavelengths.append(len(wavelengths) + 1)\n                    except:\n                        wavelengths = None\n                \n                test_quantile_preds = make_pyro_predictions(\n                    model, guide, test_features_df, feature_names, wavelengths\n                )\n                \n            except Exception as e:\n                print(f\"Error loading Pyro model: {e}\")\n                print(\"Falling back to XGBoost...\")\n                config.MODEL_TYPE = 'xgboost'\n        \n        if config.MODEL_TYPE == 'xgboost':\n            # Load XGBoost models (your existing code)\n            with open(os.path.join(config.OUTPUT_PATH, 'feature_columns.pkl'), 'rb') as f:\n                train_cols = pickle.load(f)\n            \n            test_features_df = test_features_df[train_cols]\n            \n            trained_models = {}\n            for q in config.QUANTILES:\n                model_path = os.path.join(config.OUTPUT_PATH, f'model_quantile_{q}.pkl')\n                with open(model_path, 'rb') as f:\n                    trained_models[q] = pickle.load(f)\n            \n            test_quantile_preds = {}\n            for q in config.QUANTILES:\n                test_quantile_preds[q] = trained_models[q].predict(test_features_df)\n\n        # Create submission\n        y_pred_test = test_quantile_preds[0.50].clip(0, None)\n        lower_test, upper_test = test_quantile_preds[0.05], test_quantile_preds[0.95]\n        \n        sigma_raw_test = (upper_test - lower_test) / 3.29\n        sigma_raw_test[sigma_raw_test < 0] = 1e-10\n        \n        # For Pyro models, uncertainties are better calibrated, but we can still apply minimal scaling\n        if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n            sigma_pred_test = sigma_raw_test * 1.1  # Minimal adjustment for Pyro\n        else:\n            # Load calibration for XGBoost\n            try:\n                with open(os.path.join(config.OUTPUT_PATH, 'calibration_params.pkl'), 'rb') as f:\n                    calibration_params = pickle.load(f)\n                sigma_pred_test = (sigma_raw_test * calibration_params['scaling']) + calibration_params['additive']\n            except:\n                sigma_pred_test = sigma_raw_test * 1.2  # Default scaling\n        \n        # Format submission\n        pred_df = pd.DataFrame(y_pred_test, index=sample_submission.index, columns=target_column_names)\n        sigma_df = pd.DataFrame(sigma_pred_test, index=sample_submission.index, \n                               columns=[f\"sigma_{i+1}\" for i in range(len(target_column_names))])\n        submission_df = pd.concat([pred_df, sigma_df], axis=1)\n        \n        submission_df.to_csv('submission.csv')\n        print(\"\\\\n'submission.csv' created successfully!\")\n        print(f\"\\\\nSubmission preview ({config.MODEL_TYPE}):\")\n        print(submission_df.head())\n\n    else:\n        # --- TRAINING MODE ---\n        print(\"\\\\n\" + \"=\"*60)\n        print(\"======          TRAINING MODE           ======\")\n        print(\"=\"*60 + \"\\\\n\")\n        \n        # Load training data\n        train_labels_df = pd.read_csv(config.TRAIN_LABELS_PATH, index_col='planet_id')\n        train_star_info_df = pd.read_csv(config.TRAIN_STAR_INFO_PATH, index_col='planet_id').loc[train_labels_df.index]\n        \n        print(\"Loading pre-processed training data...\")\n        f_raw_train, a_raw_train = np.load(config.F_RAW_PATH), np.load(config.A_RAW_PATH)\n        \n        train_features_df = maximal_feature_engineering(f_raw_train, a_raw_train, train_star_info_df)\n        train_labels = train_labels_df.values\n        naive_mu_train, naive_sigma_train = np.mean(train_labels), np.std(train_labels)\n\n        # Train-validation split\n        X_train, X_val, y_train, y_val = train_test_split(\n            train_features_df, train_labels, \n            test_size=config.VALIDATION_SPLIT, \n            random_state=config.RANDOM_STATE\n        )\n        \n        feature_names = train_features_df.columns.tolist()\n        \n        # Get wavelengths for physics model\n        wavelengths = None\n        if config.MODEL_TYPE == 'physics_pyro' and config.USE_WAVELENGTH_PHYSICS:\n            try:\n                wavelengths_df = pd.read_csv(config.WAVELENGTHS_PATH)\n                wavelengths = []\n                for col in wavelengths_df.columns:\n                    if col.startswith('wl_'):\n                        try:\n                            wl_val = float(col.replace('wl_', ''))\n                            wavelengths.append(wl_val)\n                        except:\n                            wavelengths.append(len(wavelengths) + 1)\n            except:\n                print(\"Could not load wavelengths, using default indexing\")\n                wavelengths = None\n\n        # Train model based on selection\n        if config.MODEL_TYPE in ['simple_pyro', 'physics_pyro']:\n            # Train Pyro model\n            model, guide, val_quantile_preds = train_pyro_model(\n                X_train, y_train, X_val, y_val, feature_names, wavelengths\n            )\n            \n            # Evaluate\n            y_pred_val = val_quantile_preds[0.50]\n            lower_val, upper_val = val_quantile_preds[0.05], val_quantile_preds[0.95]\n            sigma_raw_val = (upper_val - lower_val) / 3.29\n            sigma_raw_val[sigma_raw_val < 0] = 1e-10\n            \n            # Minimal calibration for Pyro (they're usually well-calibrated)\n            sigma_val = sigma_raw_val * 1.1\n            \n            pyro_score = official_competition_score(\n                y_val, y_pred_val, sigma_val, naive_mu_train, naive_sigma_train\n            )\n            \n            print(f\"\\\\n{config.MODEL_TYPE.upper()} Validation Score: {pyro_score:.4f}\")\n            \n            # Save models\n            save_models(model, guide, feature_names, config.OUTPUT_PATH)\n            # First model's prediction and evaluation\n            y_pred_val = val_quantile_preds[0.50]\n            lower_val, upper_val = val_quantile_preds[0.05], val_quantile_preds[0.95]\n            sigma_raw_val = (upper_val - lower_val) / 3.29\n            sigma_raw_val[sigma_raw_val < 0] = 1e-10\n            sigma_val = sigma_raw_val * 1.1\n            \n            print(\"DIAGNOSTICS AS PER CHAT 1:\")\n            abs_err = np.abs(y_val - y_pred_val)\n            normed_err = abs_err / sigma_val\n            print(\"Mean absolute error:\", abs_err.mean())\n            print(\"Mean normalized error:\", normed_err.mean())\n            print(\"Max normalized error:\", normed_err.max())\n            print(\"Min normalized error:\", normed_err.min())\n            print(\"END DIAGNOSTICS AS PER CHAT 1\")\n\n            import matplotlib.pyplot as plt\n\n            plt.scatter(y_pred_val, sigma_val, alpha=0.5)\n            plt.xlabel(\"Prediction\")\n            plt.ylabel(\"Predicted σ\")\n            plt.title(\"Prediction vs Uncertainty\")\n            plt.grid(True)\n            plt.show()\n\n            \n            pyro_score = official_competition_score(\n                y_val, y_pred_val, sigma_val, naive_mu_train, naive_sigma_train\n            )\n            \n            print(f\"\\nPYRO Validation Score: {pyro_score:.4f}\")\n            print(\"DIAGNOSTICS AS PER CHAT 2:\")\n            print(\"Mean pred:\", y_pred_val.mean().item())\n            print(\"Std pred:\", y_pred_val.std().item())\n            print(\"Any NaN?\", np.isnan(y_pred_val).any())\n            print(\"y_val mean/std:\", y_val.mean().item(), y_val.std().item())\n            print(\"Pred mean/std:\", y_pred_val.mean().item(), y_pred_val.std().item())\n            print(\"Sigma mean:\", sigma_val.mean().item())\n            print(y_pred_val[:5])\n            print(y_val[:5])\n            print(\"END DIAGNOSTICS AS PER CHAT 2\")\n\n            # Retrain on full dataset\n            print(\"\\\\nRetraining on full dataset...\")\n            full_model, full_guide, val_quantile_preds = train_pyro_model(\n                train_features_df, train_labels, \n                train_features_df[:100], train_labels[:100],  # Dummy validation for interface\n                feature_names, wavelengths\n            )\n            \n            # Save final models\n            save_models(full_model, full_guide, feature_names, config.OUTPUT_PATH)\n            \n        else:  # XGBoost\n            val_quantile_preds = train_xgboost_quantiles(X_train, y_train, X_val, y_val)\n            \n            # Your existing calibration logic\n            y_pred_val, lower_val, upper_val = val_quantile_preds[0.50], val_quantile_preds[0.05], val_quantile_preds[0.95]\n            sigma_raw_val = (upper_val - lower_val) / 3.29\n            sigma_raw_val[sigma_raw_val < 0] = 1e-10\n\n            print(\"\\\\nSearching for calibration factors...\")\n            best_score, best_scaling, best_additive = -1.0, 1.0, 0.0\n            \n            for scaling in [1.0, 1.2, 1.5, 2.0, 2.5]:\n                for additive in [0.0, 0.0005, 0.001, 0.0015]:\n                    sigma_calibrated = (sigma_raw_val * scaling) + additive\n                    score = official_competition_score(y_val, y_pred_val, sigma_calibrated, naive_mu_train, naive_sigma_train)\n                    \n                    if score > best_score:\n                        best_score, best_scaling, best_additive = score, scaling, additive\n            \n            print(f\"Best XGBoost Score: {best_score:.4f} (Scale={best_scaling}, Add={best_additive})\")\n            \n            # Save calibration and retrain models\n            os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n            calibration_params = {'scaling': best_scaling, 'additive': best_additive}\n            with open(os.path.join(config.OUTPUT_PATH, 'calibration_params.pkl'), 'wb') as f:\n                pickle.dump(calibration_params, f)\n            \n            with open(os.path.join(config.OUTPUT_PATH, 'feature_columns.pkl'), 'wb') as f:\n                pickle.dump(feature_names, f)\n\n            print(\"\\\\nRetraining XGBoost on full dataset...\")\n            for q in config.QUANTILES:\n                model = xgb.XGBRegressor(**config.XGB_PARAMS, objective='reg:quantileerror', quantile_alpha=q)\n                wrapper = MultiOutputRegressor(model, n_jobs=-1)\n                wrapper.fit(train_features_df, train_labels)\n                \n                model_path = os.path.join(config.OUTPUT_PATH, f'model_quantile_{q}.pkl')\n                with open(model_path, 'wb') as f:\n                    pickle.dump(wrapper, f)\n        \n        print(\"\\\\nTraining complete!\")\n\nif __name__ == '__main__':\n    \n    # Clear GPU memory at start\n    if config.USE_GPU and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    try:\n        main()\n    finally:\n        # Clean up GPU memory\n        if config.USE_GPU and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            print(f\"\\\\nFinal GPU memory usage: {torch.cuda.memory_allocated(device) / 1e9:.1f} GB\")\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-05T15:00:35.638204Z","iopub.execute_input":"2025-08-05T15:00:35.638521Z","iopub.status.idle":"2025-08-05T17:30:51.155240Z","shell.execute_reply.started":"2025-08-05T15:00:35.638500Z","shell.execute_reply":"2025-08-05T17:30:51.154089Z"},"papermill":{"duration":38.472562,"end_time":"2025-06-28T07:57:09.008643","exception":false,"start_time":"2025-06-28T07:56:30.536081","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Using CPU\n\\nUsing model: physics_pyro\nDevice: cpu\n\\n============================================================\n======          TRAINING MODE           ======\n============================================================\\n\nLoading pre-processed training data...\nEngineering features...\nCreated 40 features in total.\n\n--- Training PHYSICS_PYRO model ---\nModel created successfully\n=== Trying AutoMultivariateNormal ===\nGuide parameters: 3\n✅ AutoMultivariateNormal SUCCESS! Test loss: 524537823244.4052\n\n🎉 Setup complete using: AutoMultivariateNormal\nTraining for 100000 epochs...\n  Epoch    0, Loss: 5258190258190.84\n  Epoch  100, Loss: 2044485042188.47\n  Epoch  200, Loss: 766546935821.17\n  Epoch  300, Loss: 1139789660172.43\n  Epoch  400, Loss: 2478262976534.69\n  Epoch  500, Loss: 642784952336.61\n  Epoch  600, Loss: 10529590476810.48\n  Epoch  700, Loss: 583510065170.04\n  Epoch  800, Loss: 1867973394452.22\n  Epoch  900, Loss: 628619542531.87\n  Epoch 1000, Loss: 1865324953611.25\n  Epoch 1100, Loss: 664317001738.62\n  Epoch 1200, Loss: 292061970441.62\n  Epoch 1300, Loss: 1666881290254.28\n  Epoch 1400, Loss: 226555822094.81\n  Epoch 1500, Loss: 276301185041.65\n  Epoch 1600, Loss: 400590929935.29\n  Epoch 1700, Loss: 509189881869.06\n  Epoch 1800, Loss: 343088431116.25\n  Epoch 1900, Loss: 253569449994.81\n  Epoch 2000, Loss: 269920288787.44\n  Epoch 2200, Loss: 324771250188.97\n  Epoch 2300, Loss: 52257382414.74\n  Epoch 2400, Loss: 85870706694.62\n  Epoch 2500, Loss: 176092315662.46\n  Epoch 2600, Loss: 376193875978.45\n  Epoch 2700, Loss: 96517464073.67\n  Epoch 2800, Loss: 122381516812.65\n  Epoch 2900, Loss: 47752912915.10\n  Epoch 3000, Loss: 104454766607.76\n  Epoch 3100, Loss: 181755772938.10\n  Epoch 3200, Loss: 56650166280.30\n  Epoch 3300, Loss: 448933134349.78\n  Epoch 3400, Loss: 471980900361.90\n  Epoch 3500, Loss: 40398409751.99\n  Epoch 3600, Loss: 37663367179.12\n  Epoch 3700, Loss: 129204936716.35\n  Epoch 3800, Loss: 77687095307.66\n  Epoch 3900, Loss: 147709919248.13\n  Epoch 4000, Loss: 416937017354.52\n  Epoch 4100, Loss: 50648186903.76\n  Epoch 4200, Loss: 19498299395.67\n  Epoch 4300, Loss: 134015795213.63\n  Epoch 4400, Loss: 71633715208.72\n  Epoch 4500, Loss: 39790628869.91\n  Epoch 4600, Loss: 167779762182.35\n  Epoch 4700, Loss: 46195212305.29\n  Epoch 4800, Loss: 8484598799.99\n  Epoch 4900, Loss: 7100707338.12\n  Epoch 5000, Loss: 7685939729.33\n  Epoch 5100, Loss: 13395375128.27\n  Epoch 5200, Loss: 52828073997.22\n  Epoch 5300, Loss: 51093725195.99\n  Epoch 5400, Loss: 5796685829.58\n  Epoch 5500, Loss: 4261924878.37\n  Epoch 5600, Loss: 7261580305.38\n  Epoch 5700, Loss: 3826916625.95\n  Epoch 5800, Loss: 11900868621.89\n  Epoch 5900, Loss: 12657905672.58\n  Epoch 6000, Loss: 2047530247.96\n  Epoch 6100, Loss: 1961815189.31\n  Epoch 6200, Loss: 4052256523.26\n  Epoch 6300, Loss: 2231302669.33\n  Epoch 6400, Loss: 106089988109.81\n  Epoch 6500, Loss: 25812371483.70\n  Epoch 6600, Loss: 33329721362.58\n  Epoch 6700, Loss: 1718277257.67\n  Epoch 6800, Loss: 2659145999.81\n  Epoch 6900, Loss: 2639760653.73\n  Epoch 7000, Loss: 23432138769.12\n  Epoch 7100, Loss: 1048199948.93\n  Epoch 7200, Loss: 25123285000.22\n  Epoch 7300, Loss: 14990039057.26\n  Epoch 7400, Loss: 16426134547.32\n  Epoch 7500, Loss: 4936530446.67\n  Epoch 7600, Loss: 1583993228.18\n  Epoch 7700, Loss: 13893169170.38\n  Epoch 7800, Loss: 5874823182.24\n  Epoch 7900, Loss: 4951228944.62\n  Epoch 8000, Loss: 3182709764.47\n  Epoch 8100, Loss: 1619782666.82\n  Epoch 8200, Loss: 402111955.00\n  Epoch 8300, Loss: 8348841484.05\n  Epoch 8400, Loss: 3002668818.99\n  Epoch 8500, Loss: 108367512.88\n  Epoch 8600, Loss: 2434851600.00\n  Epoch 8700, Loss: 905688583.73\n  Epoch 8800, Loss: 1995908624.82\n  Epoch 8900, Loss: 5688559619.82\n  Epoch 9000, Loss: 1760150791.71\n  Epoch 9100, Loss: 3862930189.15\n  Epoch 9200, Loss: 214941455.02\n  Epoch 9300, Loss: 4613593104.01\n  Epoch 9400, Loss: 146292720.46\n  Epoch 9500, Loss: 50210992.82\n  Epoch 9600, Loss: 484333069.82\n  Epoch 9700, Loss: 1588642572.00\n  Epoch 9800, Loss: 661795857.89\n  Epoch 9900, Loss: 3993373197.89\n  Epoch 10000, Loss: 136859991.22\n  Epoch 10100, Loss: 650160460.91\n  Epoch 10200, Loss: 1065343373.95\n  Epoch 10300, Loss: 54037645.39\n  Epoch 10400, Loss: 22615704.83\n  Epoch 10500, Loss: 732724109.51\n  Epoch 10600, Loss: 366344166.02\n  Epoch 10700, Loss: 1354469010.15\n  Epoch 10800, Loss: 2613860370.54\n  Epoch 10900, Loss: 900860294.25\n  Epoch 11000, Loss: 55657151.57\n  Epoch 11100, Loss: 1053416397.52\n  Epoch 11200, Loss: 552231499.58\n  Epoch 11300, Loss: 56626999.40\n  Epoch 11400, Loss: 8251029517.62\n  Epoch 11500, Loss: 214527079.74\n  Epoch 11600, Loss: 144602738.96\n  Epoch 11700, Loss: 2389790992.99\n  Epoch 11800, Loss: 4345955347.19\n  Epoch 11900, Loss: 712822482.61\n  Epoch 12000, Loss: 187093392.96\n  Epoch 12100, Loss: 36081244.81\n  Epoch 12200, Loss: 808470669.20\n  Epoch 12300, Loss: 50831136.98\n  Epoch 12400, Loss: 522709302.35\n  Epoch 12500, Loss: 80254026.08\n  Epoch 12600, Loss: 196290053.52\n  Epoch 12700, Loss: 565205648.42\n  Epoch 12800, Loss: 911929930.95\n  Epoch 12900, Loss: 479244144.83\n  Epoch 13000, Loss: 384895182.32\n  Epoch 13100, Loss: 134132775.55\n  Epoch 13200, Loss: 2265437446.65\n  Epoch 13300, Loss: 14537316.28\n  Epoch 13400, Loss: 93294182.08\n  Epoch 13500, Loss: 110517803.42\n  Epoch 13600, Loss: 3257262613.25\n  Epoch 13700, Loss: 2848224.90\n  Epoch 13800, Loss: 233431990.65\n  Epoch 13900, Loss: 190120128.15\n  Epoch 14000, Loss: 540717845.98\n  Epoch 14100, Loss: 62516666.63\n  Epoch 14200, Loss: 4341410839.36\n  Epoch 14300, Loss: 229067.23\n  Epoch 14400, Loss: 221141545.67\n  Epoch 14500, Loss: 35951571.12\n  Epoch 14600, Loss: 13555942.14\n  Epoch 14700, Loss: 264754644.70\n  Epoch 14800, Loss: 52598255.06\n  Epoch 14900, Loss: 1225770.43\n  Epoch 15000, Loss: 41847748.32\n  Epoch 15100, Loss: 904588180.79\n  Epoch 15200, Loss: 1559489930.09\n  Epoch 15300, Loss: 2727990282.66\n  Epoch 15400, Loss: 335064.03\n  Epoch 15500, Loss: 104627573.22\n  Epoch 15600, Loss: 20421784.24\n  Epoch 15700, Loss: 447120463.34\n  Epoch 15800, Loss: 92951856.71\n  Epoch 15900, Loss: 1858790.02\n  Epoch 16000, Loss: 43706639.78\n  Epoch 16100, Loss: 12859848.07\n  Epoch 16200, Loss: 75695122.00\n  Epoch 16300, Loss: 6653840.08\n  Epoch 16400, Loss: 67896688.20\n  Epoch 16500, Loss: 98761481.92\n  Epoch 16600, Loss: 51398.79\n  Epoch 16700, Loss: 273214.11\n  Epoch 16800, Loss: 50320142.78\n  Epoch 16900, Loss: 28509098.84\n  Epoch 17000, Loss: 905480586.99\n  Epoch 17100, Loss: 28244710.02\n  Epoch 17200, Loss: 239486179.38\n  Epoch 17300, Loss: 39241604.49\n  Epoch 17400, Loss: 310392462.22\n  Epoch 17500, Loss: 11283588.88\n  Epoch 17600, Loss: 295782060.70\n  Epoch 17700, Loss: 692093969.19\n  Epoch 17800, Loss: 519516.20\n  Epoch 17900, Loss: 292317.19\n  Epoch 18000, Loss: 21140913.95\n  Epoch 18100, Loss: 238654.81\n  Epoch 18200, Loss: 578366.73\n  Epoch 18300, Loss: 18399550.44\n  Epoch 18400, Loss: 172480594.88\n  Epoch 18500, Loss: 1726301.04\n  Epoch 18600, Loss: 23816486.58\n  Epoch 18700, Loss: 69662502.79\n  Epoch 18800, Loss: 1017655.22\n  Epoch 18900, Loss: 4151252.01\n  Epoch 19000, Loss: 15876914.86\n  Epoch 19100, Loss: 903366.75\n  Epoch 19200, Loss: 119438803.23\n  Epoch 19300, Loss: 96387.17\n  Epoch 19400, Loss: 290534.00\n  Epoch 19500, Loss: 153980090.35\n  Epoch 19600, Loss: 34268889.23\n  Epoch 20100, Loss: 14233760.34\n  Epoch 20200, Loss: 415935.49\n  Epoch 20300, Loss: 7307184.73\n  Epoch 20400, Loss: 24193622.16\n  Epoch 20500, Loss: 18478795.13\n  Epoch 20600, Loss: 14923519.61\n  Epoch 20700, Loss: 7044624.60\n  Epoch 20800, Loss: 476644.08\n  Epoch 20900, Loss: 323750.08\n  Epoch 21000, Loss: 2258120.19\n  Epoch 21100, Loss: 1856178.83\n  Epoch 21200, Loss: 1386622.07\n  Epoch 21300, Loss: 19442907.25\n  Epoch 21400, Loss: 466377.52\n  Epoch 21500, Loss: 29619070.27\n  Epoch 21600, Loss: 359572.34\n  Epoch 21700, Loss: 23003735.30\n  Epoch 21800, Loss: 258830.88\n  Epoch 21900, Loss: 919787.23\n  Epoch 22000, Loss: 463588.25\n  Epoch 22100, Loss: 109869.31\n  Epoch 22200, Loss: 170565.66\n  Epoch 22300, Loss: 673364.54\n  Epoch 22400, Loss: 35320806.81\n  Epoch 22500, Loss: 36462.55\n  Epoch 23400, Loss: 177344.61\n  Epoch 23500, Loss: 453845.12\n  Epoch 23600, Loss: 412783.50\n  Epoch 23700, Loss: 913015.75\n  Epoch 23800, Loss: 519259241.14\n  Epoch 23900, Loss: 1937578.81\n  Epoch 24000, Loss: 352178.29\n  Epoch 24100, Loss: 102213228.05\n  Epoch 24200, Loss: 154936.62\n  Epoch 24300, Loss: 950611.32\n  Epoch 24400, Loss: 746223.90\n  Epoch 24500, Loss: 1426689.39\n  Epoch 24600, Loss: 71273.65\n  Epoch 24700, Loss: 590675.31\n  Epoch 24800, Loss: 94486.61\n  Epoch 24900, Loss: 571991.81\n  Epoch 25000, Loss: 23344436.09\n  Epoch 25100, Loss: 10700953.76\n  Epoch 25200, Loss: 720251.01\n  Epoch 25300, Loss: 83025123.05\n  Epoch 25400, Loss: 380595.94\n  Epoch 25500, Loss: 417685.23\n  Epoch 25600, Loss: 446891.71\n  Epoch 25700, Loss: 75457.83\n  Epoch 25800, Loss: 34795922.75\n  Epoch 25900, Loss: 21606418.38\n  Epoch 26000, Loss: 101006266.64\n  Epoch 26100, Loss: 1917989.17\n  Epoch 26200, Loss: 5033060.24\n  Epoch 26300, Loss: 2750170.43\n  Epoch 26400, Loss: 2494876.10\n  Epoch 26500, Loss: 543684.13\n  Epoch 26600, Loss: 362525.31\n  Epoch 26700, Loss: 491463.96\n  Epoch 26800, Loss: 202296.36\n  Epoch 26900, Loss: 205234.48\n  Epoch 27000, Loss: 322984.53\n  Epoch 27100, Loss: 362779.57\n  Epoch 27200, Loss:  3905.54\n  Epoch 27300, Loss: 2376649.47\n  Epoch 27400, Loss: 209475.81\n  Epoch 27500, Loss: 196071.10\n  Epoch 27600, Loss: 497349.80\n  Epoch 27700, Loss: 1328771.26\n  Epoch 27800, Loss: 103958.48\n  Epoch 27900, Loss: 803574.48\n  Epoch 28000, Loss: 6829815.09\n  Epoch 28100, Loss: -6954.35\n  Epoch 28200, Loss: 1705584.91\n  Epoch 28300, Loss: 353224.05\n  Epoch 28400, Loss: 922498.22\n  Epoch 28500, Loss: 889421.18\n  Epoch 28600, Loss: 3213375.83\n  Epoch 28700, Loss: 14620539.55\n  Epoch 28800, Loss: 252419.21\n  Epoch 28900, Loss: 290646.47\n  Epoch 29000, Loss: 1393840.10\n  Epoch 29100, Loss: 276717.52\n  Epoch 29200, Loss: 432709462.07\n  Epoch 29300, Loss: 105745.33\n  Epoch 29400, Loss: 408515.72\n  Epoch 29500, Loss: 688110.15\n  Epoch 29600, Loss: 1042353.11\n  Epoch 29700, Loss: 1004888.15\n  Epoch 29800, Loss: 1158505.24\n  Epoch 29900, Loss: 29158746.68\n  Epoch 30000, Loss: 1020804.38\n  Epoch 30100, Loss: 138604.67\n  Epoch 30200, Loss: 35893.39\n  Epoch 30300, Loss: 84352.54\n  Epoch 30400, Loss: 10880535.86\n  Epoch 30500, Loss: 868996.48\n  Epoch 30600, Loss: 903467.50\n  Epoch 30700, Loss: 2810205.60\n  Epoch 30800, Loss: 1046080.35\n  Epoch 30900, Loss: 6573407.64\n  Epoch 31000, Loss: 470487.70\n  Epoch 31100, Loss: 368668.60\n  Epoch 31200, Loss: 2042798.82\n  Epoch 31300, Loss: 777999.56\n  Epoch 31400, Loss: 253043.06\n  Epoch 31500, Loss: 2636626.45\n  Epoch 31600, Loss: 198913.02\n  Epoch 31700, Loss: 204915.20\n  Epoch 31800, Loss: 10481692.06\n  Epoch 31900, Loss: 307249.76\n  Epoch 32000, Loss: 132248.30\n  Epoch 32100, Loss: 430698.77\n  Epoch 32200, Loss: 612632.08\n  Epoch 32300, Loss: 104212.83\n  Epoch 32400, Loss: 267835.68\n  Epoch 32500, Loss: 537088.63\n  Epoch 32600, Loss: 305582.87\n  Epoch 32700, Loss: 513556.08\n  Epoch 32800, Loss: 639625.53\n  Epoch 32900, Loss: 117654.64\n  Epoch 33000, Loss: 400506.24\n  Epoch 33100, Loss: 184289.21\n  Epoch 33200, Loss: 1788699.13\n  Epoch 33300, Loss:  1058.44\n  Epoch 33400, Loss: 60103.00\n  Epoch 33500, Loss: 131217.38\n  Epoch 33600, Loss: 172963.85\n  Epoch 33700, Loss: 16817.23\n  Epoch 33800, Loss: 259807.58\n  Epoch 33900, Loss: 102392.33\n  Epoch 34000, Loss: -5775.89\n  Epoch 34100, Loss: 548757.16\n  Epoch 34200, Loss: 22983.61\n  Epoch 34300, Loss: 517082.89\n  Epoch 34400, Loss:  2718.24\n  Epoch 34500, Loss: 167271.12\n  Epoch 34600, Loss:  8657.14\n  Epoch 34700, Loss: 204131.91\n  Epoch 34800, Loss: 84187.49\n  Epoch 34900, Loss: 213280.24\n  Epoch 35000, Loss: 405973.53\n  Epoch 35100, Loss: 48326642.59\n  Epoch 35200, Loss: 1082566.12\n  Epoch 35300, Loss: 140532.40\n  Epoch 35400, Loss: 612283.19\n  Epoch 35500, Loss: 28028.32\n  Epoch 35600, Loss: 38616.52\n  Epoch 35700, Loss: 70578.34\n  Epoch 35800, Loss: 166898.02\n  Epoch 35900, Loss: 51215.18\n  Epoch 36000, Loss: 19824070.56\n  Epoch 36100, Loss: 137578.85\n  Epoch 36200, Loss: 405897.45\n  Epoch 36300, Loss: 1982273.27\n  Epoch 36400, Loss: 414843.51\n  Epoch 36500, Loss: 39122.07\n  Epoch 36600, Loss: 25645.08\n  Epoch 36700, Loss: 89208.28\n  Epoch 36800, Loss: 252335.45\n  Epoch 36900, Loss: 822735.75\n  Epoch 37000, Loss: 656067.68\n  Epoch 37100, Loss: 265507.48\n  Epoch 37200, Loss: 27145.86\n  Epoch 37300, Loss: 807945.49\n  Epoch 37400, Loss: 33124.87\n  Epoch 37500, Loss: -6393.08\n  Epoch 37600, Loss: 143560.62\n  Epoch 37700, Loss: 31708.89\n  Epoch 37800, Loss: 238846.01\n  Epoch 37900, Loss: -2989.29\n  Epoch 38000, Loss: 84798.69\n  Epoch 38100, Loss: 253523.22\n  Epoch 38200, Loss: 19190.84\n  Epoch 38300, Loss: 122436.75\n  Epoch 38400, Loss: 65113.49\n  Epoch 38500, Loss: 471299.51\n  Epoch 38600, Loss: 789109.12\n  Epoch 38700, Loss: 760277.32\n  Epoch 38800, Loss: 38159.41\n  Epoch 38900, Loss: 386131.81\n  Epoch 39000, Loss: 30539.28\n  Epoch 39100, Loss: 108621.55\n  Epoch 39200, Loss: 145279.14\n  Epoch 39300, Loss: 663960.30\n  Epoch 39400, Loss: 84647.08\n  Epoch 39500, Loss: 124524.86\n  Epoch 39600, Loss: 82079.31\n  Epoch 39700, Loss: 412892.29\n  Epoch 39800, Loss: 16206.63\n  Epoch 39900, Loss: 369910.49\n  Epoch 40000, Loss: 12797.39\n  Epoch 40100, Loss: 327539.35\n  Epoch 40200, Loss: 39380.52\n  Epoch 40300, Loss: 95164.84\n  Epoch 40400, Loss: 160970.97\n  Epoch 40500, Loss: 220099.34\n  Epoch 40600, Loss: 222673.32\n  Epoch 40700, Loss: 459756.79\n  Epoch 40800, Loss: 95881.39\n  Epoch 40900, Loss:  5261.89\n  Epoch 41000, Loss: 63232.37\n  Epoch 41100, Loss: 313099.64\n  Epoch 41200, Loss:  3872.27\n  Epoch 41300, Loss: 105149.73\n  Epoch 41400, Loss: 136353.10\n  Epoch 41500, Loss: -12310.86\n  Epoch 41600, Loss: 325263.22\n  Epoch 41700, Loss: 545534.81\n  Epoch 41800, Loss: 66254.16\n  Epoch 41900, Loss: 302279.57\n  Epoch 42000, Loss: 164256.27\n  Epoch 42100, Loss: 10850450.87\n  Epoch 42200, Loss: 400847.21\n  Epoch 42300, Loss: 3636482.36\n  Epoch 42400, Loss: -15111.60\n  Epoch 42500, Loss: 67810.44\n  Epoch 42600, Loss: 51818930.41\n  Epoch 42700, Loss: 84374.29\n  Epoch 42800, Loss: 34296.96\n  Epoch 42900, Loss: 1151113.76\n  Epoch 43000, Loss: 35478.24\n  Epoch 43100, Loss: 35927.76\n  Epoch 43200, Loss: 226499.84\n  Epoch 43300, Loss: 37386.22\n  Epoch 43400, Loss: 127446.44\n  Epoch 43500, Loss: -5097.00\n  Epoch 43600, Loss: 83305.46\n  Epoch 43700, Loss: -8371.99\n  Epoch 43800, Loss: 40981.41\n  Epoch 43900, Loss: 220836.15\n  Epoch 44000, Loss: 33289.07\n  Epoch 44100, Loss: 90331.99\n  Epoch 44200, Loss: 153090.92\n  Epoch 44300, Loss:  6343.57\n  Epoch 44400, Loss: 68408.80\n  Epoch 44500, Loss:   226.50\n  Epoch 44600, Loss: 23139787.68\n  Epoch 44700, Loss: 138470.75\n  Epoch 44800, Loss: 199775.84\n  Epoch 44900, Loss: 149582.01\n  Epoch 45000, Loss: 278113619.77\n  Epoch 45100, Loss: 16095.10\n  Epoch 45200, Loss: 38916.89\n  Epoch 45300, Loss: 149041.35\n  Epoch 45400, Loss: -11979.37\n  Epoch 45500, Loss: 95723.21\n  Epoch 45600, Loss: 14658.83\n  Epoch 45700, Loss: 10149.54\n  Epoch 45800, Loss: 116539.12\n  Epoch 45900, Loss: 22469599.69\n  Epoch 46000, Loss: 214854.21\n  Epoch 46100, Loss: 87756.03\n  Epoch 46200, Loss: 34127.82\n  Epoch 46300, Loss: 80779.51\n  Epoch 46400, Loss: 47497.70\n  Epoch 46500, Loss: 156850.42\n  Epoch 46600, Loss: -1765.35\n  Epoch 46700, Loss:  8526.94\n  Epoch 46800, Loss: 74222.53\n  Epoch 46900, Loss: 665561.08\n  Epoch 47000, Loss: -16314.05\n  Epoch 47100, Loss:  5836.90\n  Epoch 47200, Loss: -11024.69\n  Epoch 47300, Loss: 22362.62\n  Epoch 47400, Loss: -8248.16\n  Epoch 47500, Loss: -12508.67\n  Epoch 47600, Loss: 39592.01\n  Epoch 47700, Loss: -8344.02\n  Epoch 47800, Loss: 17204.63\n  Epoch 47900, Loss: -16254.94\n  Epoch 48000, Loss: 28300.98\n  Epoch 48100, Loss: 40173.94\n  Epoch 48200, Loss: 26544.79\n  Epoch 48300, Loss: 193319.32\n  Epoch 48400, Loss: 73082.02\n  Epoch 48500, Loss: 414165.56\n  Epoch 48600, Loss: 626169.03\n  Epoch 48700, Loss: 11602.86\n  Epoch 48800, Loss: 36285.48\n  Epoch 48900, Loss: 379142.38\n  Epoch 49000, Loss: 57165.35\n  Epoch 49100, Loss: 143409.66\n  Epoch 49200, Loss: -11681.41\n  Epoch 49300, Loss: -7625.69\n  Epoch 49400, Loss: 74547.09\n  Epoch 49500, Loss:  5376.37\n  Epoch 49600, Loss: 39988.36\n  Epoch 49700, Loss: 136708.93\n  Epoch 49800, Loss:  3293.56\n  Epoch 49900, Loss: 25506.37\n  Epoch 50000, Loss:  -552.40\n  Epoch 50100, Loss: 42971.99\n  Epoch 50200, Loss: 34536.77\n  Epoch 50300, Loss: 33417.04\n  Epoch 50400, Loss: 147994.07\n  Epoch 50500, Loss: -3081.36\n  Epoch 50600, Loss: 29862.11\n  Epoch 50700, Loss: -11304.19\n  Epoch 50800, Loss: 102008.08\n  Epoch 50900, Loss:  9167.47\n  Epoch 51000, Loss: -2489.47\n  Epoch 51100, Loss: 216754.96\n  Epoch 51200, Loss: 57745.38\n  Epoch 51300, Loss: -2127.33\n  Epoch 51400, Loss: -4877.37\n  Epoch 51500, Loss:  9373.80\n  Epoch 51600, Loss:  7970.26\n  Epoch 51700, Loss: -19365.12\n  Epoch 51800, Loss: -8442.59\n  Epoch 51900, Loss: 22123.86\n  Epoch 52000, Loss:  6597.21\n  Epoch 52100, Loss: -6254.25\n  Epoch 52200, Loss: 31722.79\n  Epoch 52300, Loss: 154547.58\n  Epoch 52400, Loss: -17885.06\n  Epoch 52500, Loss:  5916.72\n  Epoch 52600, Loss: 197717.42\n  Epoch 52700, Loss: 66295.47\n  Epoch 52800, Loss: 121195.50\n  Epoch 52900, Loss: -3135.99\n  Epoch 53000, Loss: 67468.25\n  Epoch 53100, Loss:  9707.46\n  Epoch 53200, Loss: 13027.81\n  Epoch 53300, Loss: 24940.08\n  Epoch 53400, Loss: 418938.70\n  Epoch 53500, Loss: 24313.40\n  Epoch 53600, Loss: -11563.51\n  Epoch 53700, Loss: -4894.86\n  Epoch 53800, Loss: 144814.16\n  Epoch 53900, Loss: 46425.65\n  Epoch 54000, Loss: 13022.28\n  Epoch 54100, Loss: 312502.01\n  Epoch 54200, Loss:  1176.23\n  Epoch 54300, Loss: -8685.72\n  Epoch 54400, Loss: -19493.86\n  Epoch 54500, Loss: 143961.06\n  Epoch 54600, Loss: -3379.53\n  Epoch 54700, Loss: 12971.64\n  Epoch 54800, Loss:  2615.51\n  Epoch 54900, Loss: 43929.37\n  Epoch 55000, Loss: -20959.49\n  Epoch 55100, Loss: -17918.49\n  Epoch 55200, Loss:  1300.03\n  Epoch 55300, Loss: -21263.98\n  Epoch 55400, Loss: -21913.20\n  Epoch 55500, Loss: -7082.37\n  Epoch 55600, Loss: 125599.98\n  Epoch 55700, Loss: -3961.71\n  Epoch 55800, Loss: -14778.33\n  Epoch 55900, Loss: -22813.82\n  Epoch 56000, Loss: 119947.06\n  Epoch 56100, Loss: 73228.37\n  Epoch 56200, Loss: 93113.17\n  Epoch 56300, Loss:  -204.31\n  Epoch 56400, Loss: -7903.68\n  Epoch 56500, Loss: -24123.99\n  Epoch 56600, Loss: -15046.88\n  Epoch 56700, Loss: 46156.03\n  Epoch 56800, Loss:  2141.47\n  Epoch 56900, Loss: -20407.50\n  Epoch 57000, Loss: 131203.98\n  Epoch 57100, Loss: 123995.82\n  Epoch 57200, Loss: -17683.55\n  Epoch 57300, Loss:    59.42\n  Epoch 57400, Loss: -20393.17\n  Epoch 57500, Loss: -8468.07\n  Epoch 57600, Loss: -13042.97\n  Epoch 57700, Loss: 45708.18\n  Epoch 57800, Loss: 94168.52\n  Epoch 57900, Loss: -7929.19\n  Epoch 58000, Loss: 19437.64\n  Epoch 58100, Loss: -18629.44\n  Epoch 58200, Loss: 16962.35\n  Epoch 58300, Loss: -11039.96\n  Epoch 58400, Loss: -7151.63\n  Epoch 58500, Loss: -19855.76\n  Epoch 58600, Loss: 150322.19\n  Epoch 58700, Loss: -12287.45\n  Epoch 58800, Loss: -19700.28\n  Epoch 58900, Loss: -16010.09\n  Epoch 59000, Loss: -10370.23\n  Epoch 59100, Loss: -23143.90\n  Epoch 59200, Loss: -1153.07\n  Epoch 59300, Loss:   582.95\n  Epoch 59400, Loss:  9728.89\n  Epoch 59500, Loss: -25089.37\n  Epoch 59600, Loss: -5837.92\n  Epoch 59700, Loss: 23380.25\n  Epoch 59800, Loss: -23967.80\n  Epoch 59900, Loss: 28789.30\n  Epoch 60000, Loss: -25253.30\n  Epoch 60100, Loss: -20410.91\n  Epoch 60200, Loss: -22894.69\n  Epoch 60300, Loss:  9054.49\n  Epoch 60400, Loss: -10653.28\n  Epoch 60500, Loss: -3970.38\n  Epoch 60600, Loss: -25030.39\n  Epoch 60700, Loss: -22417.92\n  Epoch 60800, Loss:  1894.09\n  Epoch 60900, Loss: -20876.57\n  Epoch 63700, Loss: 59713.94\n  Epoch 63800, Loss: -29157.71\n  Epoch 63900, Loss: 27465.17\n  Epoch 64000, Loss: 15093448.23\n  Epoch 64100, Loss: -15630.49\n  Epoch 64200, Loss: -14477.19\n  Epoch 64300, Loss: -5496.73\n  Epoch 64400, Loss:    44.47\n  Epoch 64500, Loss:  3135.20\n  Epoch 64600, Loss: -26676.73\n  Epoch 64700, Loss:  5920.79\n  Epoch 64800, Loss: 50629.82\n  Epoch 64900, Loss: -25813.94\n  Epoch 65000, Loss: 35772.98\n  Epoch 65100, Loss: -28514.28\n  Epoch 65200, Loss: 113321.42\n  Epoch 65300, Loss: -24370.47\n  Epoch 65400, Loss: -28742.61\n  Epoch 65500, Loss: -21854.69\n  Epoch 65600, Loss: -25237.28\n  Epoch 65700, Loss: -25561.42\n  Epoch 65800, Loss: -28601.70\n  Epoch 65900, Loss: -25765.68\n  Epoch 66000, Loss: -29537.70\n  Epoch 66100, Loss: -27873.13\n  Epoch 66200, Loss: -27487.18\n  Epoch 66300, Loss: -29233.22\n  Epoch 66400, Loss: -9572.76\n  Epoch 66500, Loss: 28947.37\n  Epoch 66600, Loss: -17942.00\n  Epoch 66700, Loss: -27605.50\n  Epoch 66800, Loss: -28442.52\n  Epoch 66900, Loss: -27238.22\n  Epoch 67000, Loss: -24445.65\n  Epoch 67100, Loss: -24416.29\n  Epoch 67200, Loss: -26658.38\n  Epoch 67300, Loss: 15016.81\n  Epoch 67400, Loss: -25455.95\n  Epoch 67500, Loss: 10635.28\n  Epoch 67600, Loss: -10167.83\n  Epoch 67700, Loss: -23923.89\n  Epoch 67800, Loss: -25207.40\n  Epoch 67900, Loss: -31057.33\n  Epoch 68000, Loss: -26255.45\n  Epoch 68100, Loss: 14231.14\n  Epoch 68200, Loss: -25032.16\n  Epoch 68300, Loss: -28403.20\n  Epoch 68400, Loss: -4581.39\n  Epoch 68500, Loss: -12873.20\n  Epoch 68600, Loss: -25250.61\n  Epoch 68700, Loss: -26324.74\n  Epoch 68800, Loss: -28997.38\n  Epoch 68900, Loss: -24989.27\n  Epoch 69000, Loss: -30870.21\n  Epoch 69100, Loss: 11604.97\n  Epoch 69200, Loss: 67436.63\n  Epoch 69300, Loss: -26504.37\n  Epoch 69400, Loss: -25218.67\n  Epoch 69500, Loss: -26870.74\n  Epoch 69600, Loss: -27425.93\n  Epoch 69700, Loss: -26578.17\n  Epoch 69800, Loss: -19104.97\n  Epoch 69900, Loss: -28039.77\n  Epoch 70000, Loss: -26214.02\n  Epoch 70100, Loss: -9514.16\n  Epoch 70200, Loss: -28588.88\n  Epoch 70300, Loss: -25036.12\n  Epoch 70400, Loss: -2235.39\n  Epoch 70500, Loss:  1065.55\n  Epoch 70600, Loss: -27795.32\n  Epoch 70700, Loss: -23625.32\n  Epoch 70800, Loss: -26992.48\n  Epoch 70900, Loss: -26390.90\n  Epoch 71000, Loss: -29512.95\n  Epoch 71100, Loss: -28814.19\n  Epoch 71200, Loss: -16758.37\n  Epoch 71300, Loss: -18180.52\n  Epoch 71400, Loss: -23731.38\n  Epoch 71500, Loss: -22545.74\n  Epoch 71600, Loss: -24438.87\n  Epoch 71700, Loss: -18211.55\n  Epoch 71800, Loss: 5527427.10\n  Epoch 71900, Loss: -27473.40\n  Epoch 72000, Loss: -11126.18\n  Epoch 72100, Loss: -33267.68\n  Epoch 72200, Loss: -30665.30\n  Epoch 72300, Loss: -26566.83\n  Epoch 72400, Loss: -29607.03\n  Epoch 72500, Loss: -26981.22\n  Epoch 72600, Loss: -10651.55\n  Epoch 72700, Loss: 20886.89\n  Epoch 72800, Loss: -26055.07\n  Epoch 72900, Loss: 21520.02\n  Epoch 73000, Loss: -26490.35\n  Epoch 73100, Loss:  7913.13\n  Epoch 73200, Loss: -24726.28\n  Epoch 73300, Loss: -30309.43\n  Epoch 73400, Loss: -20249.95\n  Epoch 73500, Loss: 10161.50\n  Epoch 73600, Loss: -30097.06\n  Epoch 73700, Loss: -23634.58\n  Epoch 73800, Loss: -19711.21\n  Epoch 73900, Loss: -28806.21\n  Epoch 74000, Loss: -29770.43\n  Epoch 74100, Loss: -26567.64\n  Epoch 74200, Loss: -2208.78\n  Epoch 74300, Loss: -21872.98\n  Epoch 74400, Loss: -23377.50\n  Epoch 74500, Loss: -27793.51\n  Epoch 74600, Loss: -29241.59\n  Epoch 74700, Loss: -20519.76\n  Epoch 74800, Loss: -21048.56\n  Epoch 74900, Loss: -25933.44\n  Epoch 75000, Loss: -21117.85\n  Epoch 75100, Loss: -22943.87\n  Epoch 75200, Loss: -29400.54\n  Epoch 75300, Loss: -28869.23\n  Epoch 75400, Loss: -26844.86\n  Epoch 75500, Loss: 29414.57\n  Epoch 75600, Loss: -30442.42\n  Epoch 75700, Loss:  9521.15\n  Epoch 75800, Loss: 69827.47\n  Epoch 75900, Loss: -27206.14\n  Epoch 76000, Loss: -30374.46\n  Epoch 76100, Loss: -12863.51\n  Epoch 76200, Loss: -30321.33\n  Epoch 76300, Loss: 21176.60\n  Epoch 76400, Loss: -27744.85\n  Epoch 76500, Loss: -29211.70\n  Epoch 76600, Loss: 84696.51\n  Epoch 76700, Loss: -30979.48\n  Epoch 76800, Loss: -13125.91\n  Epoch 76900, Loss: -21842.11\n  Epoch 77000, Loss: -6872.64\n  Epoch 77100, Loss: -28388.79\n  Epoch 77200, Loss: -30228.04\n  Epoch 77300, Loss: 14472.76\n  Epoch 77400, Loss: -27952.14\n  Epoch 77500, Loss: -28694.98\n  Epoch 77600, Loss: -23685.86\n  Epoch 77700, Loss: -28738.83\n  Epoch 77800, Loss: -24134.27\n  Epoch 77900, Loss: -29375.38\n  Epoch 78000, Loss: -33255.18\n  Epoch 78100, Loss: 45181.86\n  Epoch 78200, Loss: -16912.05\n  Epoch 78300, Loss: -26665.92\n  Epoch 78400, Loss: -19336.25\n  Epoch 78500, Loss: -29277.99\n  Epoch 78600, Loss: -26375.23\n  Epoch 78700, Loss: -21329.70\n  Epoch 78800, Loss: -22199.52\n  Epoch 78900, Loss: -25252.05\n  Epoch 79000, Loss: -26353.53\n  Epoch 79100, Loss: -5849.10\n  Epoch 79200, Loss: -31039.30\n  Epoch 79300, Loss: -28131.80\n  Epoch 79400, Loss: -21212.17\n  Epoch 79500, Loss: -21848.66\n  Epoch 79600, Loss: -28599.85\n  Epoch 79700, Loss: -31711.79\n  Epoch 79800, Loss: -23810.50\n  Epoch 79900, Loss: -26214.28\n  Epoch 80000, Loss: -32169.33\n  Epoch 80100, Loss: -25886.34\n  Epoch 80200, Loss: -26951.14\n  Epoch 80300, Loss: -24782.41\n  Epoch 80400, Loss: -10398.73\n  Epoch 80500, Loss: -30701.38\n  Epoch 80600, Loss: -29081.03\n  Epoch 80700, Loss: -13142.59\n  Epoch 80800, Loss: -15278.66\n  Epoch 80900, Loss: -30929.08\n  Epoch 81000, Loss: -20959.43\n  Epoch 81100, Loss: -28482.25\n  Epoch 81200, Loss: -27288.50\n  Epoch 81300, Loss: 59204.72\n  Epoch 81400, Loss: -31499.99\n  Epoch 81500, Loss: -30708.30\n  Epoch 81600, Loss: -29946.79\n  Epoch 81700, Loss: -31440.66\n  Epoch 81800, Loss: -18493.52\n  Epoch 81900, Loss: -33456.79\n  Epoch 82000, Loss: -29490.29\n  Epoch 82100, Loss: -31522.34\n  Epoch 82200, Loss: -29089.84\n  Epoch 82300, Loss: 115347.01\n  Epoch 82400, Loss: -27370.11\n  Epoch 82500, Loss: -27043.17\n  Epoch 82600, Loss: -27178.81\n  Epoch 82700, Loss: 59840.84\n  Epoch 82800, Loss: -27391.35\n  Epoch 82900, Loss: -31961.98\n  Epoch 83000, Loss: -30977.40\n  Epoch 83100, Loss: -14657.67\n  Epoch 83200, Loss: -8463.46\n  Epoch 83300, Loss: -24957.54\n  Epoch 83400, Loss: -24903.33\n  Epoch 83500, Loss:  5802.74\n  Epoch 83600, Loss: 21095.15\n  Epoch 83700, Loss: -13129.08\n  Epoch 83800, Loss: -31291.71\n  Epoch 83900, Loss: -29855.68\n  Epoch 84000, Loss: -28419.95\n  Epoch 84100, Loss: -25561.98\n  Epoch 84200, Loss: -13357.64\n  Epoch 84300, Loss: -26267.59\n  Epoch 84400, Loss: -25693.08\n  Epoch 84500, Loss:  3738.95\n  Epoch 84600, Loss: -24879.94\n  Epoch 84700, Loss: -24853.24\n  Epoch 84800, Loss: -26215.24\n  Epoch 84900, Loss: -21722.20\n  Epoch 85000, Loss: 185638.65\n  Epoch 85100, Loss: -23863.63\n  Epoch 85200, Loss:  9861.84\n  Epoch 85300, Loss: -28351.99\n  Epoch 85400, Loss: -25585.10\n  Epoch 85500, Loss: -29292.73\n  Epoch 85600, Loss: -8732.28\n  Epoch 85700, Loss: -27297.10\n  Epoch 85800, Loss: -28418.20\n  Epoch 85900, Loss: -20236.15\n  Epoch 86000, Loss: -28760.65\n  Epoch 86100, Loss: -25741.88\n  Epoch 86200, Loss: -15260.62\n  Epoch 86300, Loss: -10421.64\n  Epoch 86400, Loss: -25558.20\n  Epoch 86500, Loss: -21730.77\n  Epoch 86600, Loss: -16882.13\n  Epoch 86700, Loss: -24605.13\n  Epoch 86800, Loss: -30873.38\n  Epoch 86900, Loss: -15590.45\n  Epoch 87000, Loss: 390262.12\n  Epoch 87100, Loss: -27615.98\n  Epoch 87200, Loss: -27597.55\n  Epoch 87300, Loss: -30078.62\n  Epoch 87400, Loss: -24942.29\n  Epoch 87500, Loss: -27950.46\n  Epoch 87600, Loss: -27239.19\n  Epoch 87700, Loss: -25616.67\n  Epoch 87800, Loss: -22611.79\n  Epoch 87900, Loss: -29129.99\n  Epoch 88000, Loss: -17592.70\n  Epoch 88100, Loss: 16203.59\n  Epoch 88200, Loss: -28342.70\n  Epoch 88300, Loss: -26617.18\n  Epoch 88400, Loss: -30826.72\n  Epoch 88500, Loss: -26209.85\n  Epoch 88600, Loss: -29750.76\n  Epoch 88700, Loss: -21582.38\n  Epoch 88800, Loss: -30690.80\n  Epoch 88900, Loss: -14758.28\n  Epoch 89000, Loss: -31318.86\n  Epoch 89100, Loss: -22411.51\n  Epoch 89200, Loss: -29954.55\n  Epoch 89300, Loss: -26687.88\n  Epoch 89400, Loss: -27139.77\n  Epoch 89500, Loss: -28727.65\n  Epoch 89600, Loss: -28431.79\n  Epoch 89700, Loss: -25786.85\n  Epoch 89800, Loss: -30666.63\n  Epoch 89900, Loss: -26169.18\n  Epoch 90000, Loss: -28863.70\n  Epoch 90100, Loss: -24543.34\n  Epoch 90200, Loss: -2659.69\n  Epoch 90300, Loss: -22389.52\n  Epoch 90400, Loss: -27353.07\n  Epoch 90500, Loss: -31156.67\n  Epoch 90600, Loss: -8325.57\n  Epoch 90700, Loss: -25318.66\n  Epoch 90800, Loss: -15523.33\n  Epoch 90900, Loss: 60796.33\n  Epoch 91000, Loss: -23956.61\n  Epoch 91100, Loss: -30564.37\n  Epoch 91200, Loss: -25662.55\n  Epoch 91300, Loss: -12637.47\n  Epoch 91400, Loss: -25918.38\n  Epoch 91500, Loss: -29800.50\n  Epoch 91600, Loss: -10921.00\n  Epoch 91700, Loss: -32245.72\n  Epoch 91800, Loss: -28292.98\n  Epoch 91900, Loss: -30506.16\n  Epoch 92000, Loss: -26447.20\n  Epoch 92100, Loss: -30943.40\n  Epoch 92200, Loss: -31451.90\n  Epoch 92300, Loss: -30602.44\n  Epoch 92400, Loss: -20471.90\n  Epoch 92500, Loss: -25723.14\n  Epoch 92600, Loss: -24813.68\n  Epoch 92700, Loss: -27399.19\n  Epoch 92800, Loss: -22693.46\n  Epoch 92900, Loss: -33607.13\n  Epoch 93000, Loss: -29811.41\n  Epoch 93100, Loss: -14157.61\n  Epoch 93200, Loss: 17619.42\n  Epoch 93300, Loss: -28572.75\n  Epoch 93400, Loss: -13211.33\n  Epoch 93500, Loss: -27426.02\n  Epoch 93600, Loss:  6442.34\n  Epoch 93700, Loss: -25112.93\n  Epoch 93800, Loss: -21475.25\n  Epoch 93900, Loss: -29427.29\n  Epoch 94000, Loss:  2840.81\n  Epoch 94100, Loss: -20033.69\n  Epoch 94200, Loss: -24490.01\n  Epoch 94300, Loss: -28635.82\n  Epoch 94400, Loss: -26996.80\n  Epoch 94500, Loss: -28734.43\n  Epoch 94600, Loss:  3616.83\n  Epoch 94700, Loss: -25085.15\n  Epoch 94800, Loss: -15312.57\n  Epoch 94900, Loss: -27107.59\n  Epoch 95000, Loss: -22284.12\n  Epoch 95100, Loss: -31031.19\n  Epoch 95200, Loss: -26125.42\n  Epoch 95300, Loss: -17211.22\n  Epoch 95400, Loss: -28155.24\n  Epoch 95500, Loss: -26458.55\n  Epoch 95600, Loss: -27133.16\n  Epoch 95700, Loss: 10650.80\n  Epoch 95800, Loss: -26742.94\n  Epoch 95900, Loss: -1783.04\n  Epoch 96000, Loss: -23810.55\n  Epoch 96100, Loss: -30583.23\n  Epoch 96200, Loss:  5529.02\n  Epoch 96300, Loss: -29628.96\n  Epoch 96400, Loss: -26083.30\n  Epoch 96500, Loss: -31603.41\n  Epoch 96600, Loss: -31892.61\n  Epoch 96700, Loss: -24508.55\n  Epoch 96800, Loss: -24628.95\n  Epoch 96900, Loss: -16503.16\n  Epoch 97000, Loss: -27148.21\n  Epoch 97100, Loss: -29909.35\n  Epoch 97200, Loss: -25008.10\n  Epoch 97300, Loss: -24849.06\n  Epoch 97400, Loss: -30770.83\n  Epoch 97500, Loss:  7988.72\n  Epoch 97600, Loss: -26745.77\n  Epoch 97700, Loss: -18006.79\n  Epoch 97800, Loss: -28110.09\n  Epoch 97900, Loss: -28139.89\n  Epoch 98000, Loss: -22016.57\n  Epoch 98100, Loss: -29652.17\n  Epoch 98200, Loss: -28031.57\n  Epoch 98300, Loss: -27509.53\n  Epoch 98400, Loss: -23163.63\n  Epoch 98500, Loss:  -749.24\n  Epoch 98600, Loss: -28119.45\n  Epoch 98700, Loss: -29876.27\n  Epoch 98800, Loss:  6931.43\n  Epoch 98900, Loss: -27276.97\n  Epoch 99000, Loss: -29333.40\n  Epoch 99100, Loss: -28362.17\n  Epoch 99200, Loss: -22212.97\n  Epoch 99300, Loss:  1622.71\n  Epoch 99400, Loss: -29563.08\n  Epoch 99500, Loss: -28402.72\n  Epoch 99600, Loss: -22929.75\n  Epoch 99700, Loss: -31139.14\n  Epoch 99800, Loss: -28899.62\n  Epoch 99900, Loss:  -376.52\nGenerating validation predictions...\nTraining complete. Final loss: -11268.44\n\\nPHYSICS_PYRO Validation Score: 0.0000\nPyro models saved to: /kaggle/working/ariel-data-2025-27\nDIAGNOSTICS AS PER CHAT 1:\nMean absolute error: 0.008216789329801178\nMean normalized error: 0.7235103322384803\nMax normalized error: 14.850752530675315\nMin normalized error: 4.307824048940473e-05\nEND DIAGNOSTICS AS PER CHAT 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nPYRO Validation Score: 0.0000\nDIAGNOSTICS AS PER CHAT 2:\nMean pred: 0.014648327603936195\nStd pred: 0.0006494008121080697\nAny NaN? False\ny_val mean/std: 0.014978743488125727 0.011348901591756461\nPred mean/std: 0.014648327603936195 0.0006494008121080697\nSigma mean: 0.011454606428742409\n[[0.01488217 0.01481354 0.01567933 ... 0.01484146 0.01376397 0.01567107]\n [0.01470101 0.01554157 0.0166514  ... 0.01435766 0.01451637 0.01408207]\n [0.01539079 0.01475785 0.01497625 ... 0.01451264 0.01397856 0.01394572]\n [0.0148211  0.01545537 0.01427141 ... 0.01425044 0.01499385 0.01524027]\n [0.01483944 0.01467877 0.01482578 ... 0.0141925  0.01354257 0.01398166]]\n[[0.00580957 0.0056696  0.00566773 ... 0.00556971 0.00557163 0.00555928]\n [0.02132329 0.02150939 0.02150258 ... 0.02125517 0.02125705 0.02125592]\n [0.00518672 0.00515674 0.00515666 ... 0.00516809 0.00516835 0.00516852]\n [0.00639365 0.00638071 0.0063805  ... 0.00638365 0.00638342 0.00638337]\n [0.01639064 0.0163704  0.01637054 ... 0.01633427 0.01633436 0.01633411]]\nEND DIAGNOSTICS AS PER CHAT 2\n\\nRetraining on full dataset...\n\n--- Training PHYSICS_PYRO model ---\nModel created successfully\n=== Trying AutoMultivariateNormal ===\nGuide parameters: 3\n✅ AutoMultivariateNormal SUCCESS! Test loss: 48003481605.1845\n\n🎉 Setup complete using: AutoMultivariateNormal\nTraining for 100000 epochs...\n  Epoch    0, Loss: 993063731211.99\n  Epoch  100, Loss: 88088018955.73\n  Epoch  200, Loss: 1277560356878.10\n  Epoch  300, Loss: 673784856590.49\n  Epoch  400, Loss: 3094541762579.16\n  Epoch  500, Loss: 3662607548425.23\n  Epoch  600, Loss: 902295978002.58\n  Epoch  700, Loss: 2503704576023.60\n  Epoch  800, Loss: 766254514196.70\n  Epoch  900, Loss: 495234252816.15\n  Epoch 1000, Loss: 570427310098.77\n  Epoch 1100, Loss: 501073084430.86\n  Epoch 1200, Loss: 198233817108.67\n  Epoch 1300, Loss: 468082753554.87\n  Epoch 1400, Loss: 217459769368.65\n  Epoch 1500, Loss: 335496708112.89\n  Epoch 1600, Loss: 558407745545.83\n  Epoch 1700, Loss: 1862162579472.13\n  Epoch 1800, Loss: 413863051279.23\n  Epoch 1900, Loss: 444106375186.69\n  Epoch 2000, Loss: 889762218006.98\n  Epoch 2100, Loss: 433354735633.72\n  Epoch 2200, Loss: 142347485197.69\n  Epoch 2300, Loss: 45747310606.29\n  Epoch 2400, Loss: 487198949390.53\n  Epoch 2500, Loss: 777767288849.84\n  Epoch 2600, Loss: 50934026261.74\n  Epoch 2700, Loss: 36256137237.35\n  Epoch 2800, Loss: 31234369549.99\n  Epoch 2900, Loss: 189983129618.76\n  Epoch 3000, Loss: 130076672008.21\n  Epoch 3100, Loss: 109626556427.82\n  Epoch 3200, Loss: 332306513944.14\n  Epoch 3300, Loss: 60863291400.26\n  Epoch 3400, Loss: 55978930195.62\n  Epoch 3500, Loss: 6317734420.06\n  Epoch 3600, Loss: 50849136655.59\n  Epoch 3700, Loss: 35334176776.47\n  Epoch 3800, Loss: 11651654670.17\n  Epoch 3900, Loss: 9465329680.05\n  Epoch 4000, Loss: 133682282510.45\n  Epoch 4100, Loss: 24923475983.17\n  Epoch 4200, Loss: 47613136912.58\n  Epoch 4300, Loss: 16476823574.12\n  Epoch 4400, Loss: 7932436491.96\n  Epoch 4500, Loss: 104899674125.53\n  Epoch 4600, Loss: 3930145304.28\n  Epoch 4700, Loss: 24606818313.01\n  Epoch 4800, Loss: 63253819403.19\n  Epoch 4900, Loss: 15118578698.04\n  Epoch 5000, Loss: 22675443722.26\n  Epoch 5100, Loss: 4507178518.97\n  Epoch 5200, Loss: 70289293333.52\n  Epoch 5300, Loss: 2115504281.20\n  Epoch 5400, Loss: 13230224400.30\n  Epoch 5500, Loss: 3638616846.24\n  Epoch 5600, Loss: 12002503690.42\n  Epoch 5700, Loss: 8665299989.06\n  Epoch 5800, Loss: 6219847705.17\n  Epoch 5900, Loss: 3500861196.89\n  Epoch 6000, Loss: 18173249547.43\n  Epoch 6100, Loss: 5895239696.09\n  Epoch 6200, Loss: 16294838288.70\n  Epoch 6300, Loss: 36120649736.89\n  Epoch 6400, Loss: 7736073240.86\n  Epoch 6500, Loss: 14023393298.20\n  Epoch 6600, Loss: 64014139407.13\n  Epoch 6700, Loss: 5685046801.96\n  Epoch 6800, Loss: 3655883809.53\n  Epoch 6900, Loss: 402693978.35\n  Epoch 7000, Loss: 8584757274.76\n  Epoch 7100, Loss: 1579124890.26\n  Epoch 7200, Loss: 10756145159.79\n  Epoch 7300, Loss: 5971483674.25\n  Epoch 7400, Loss: 1474269084.06\n  Epoch 7500, Loss: 337263218.58\n  Epoch 7600, Loss: 540884241.85\n  Epoch 7700, Loss: 1418455310.39\n  Epoch 7800, Loss: 3079638538.39\n  Epoch 7900, Loss: 12982810645.73\n  Epoch 8000, Loss: 2748056078.50\n  Epoch 8100, Loss: 2470658840.82\n  Epoch 8200, Loss: 18299351065.61\n  Epoch 8300, Loss: 2818709515.13\n  Epoch 8400, Loss: 2446491155.05\n  Epoch 8500, Loss: 3488632328.24\n  Epoch 8600, Loss: 5455954961.60\n  Epoch 8700, Loss: 333261500.59\n  Epoch 8800, Loss: 2551699220.04\n  Epoch 8900, Loss: 5350550545.29\n  Epoch 9000, Loss: 1089311506.00\n  Epoch 9100, Loss: 246279453.05\n  Epoch 9200, Loss: 564775699.56\n  Epoch 9300, Loss: 269031438.71\n  Epoch 9400, Loss: 6917552662.84\n  Epoch 9500, Loss: 197000365.30\n  Epoch 9600, Loss: 2115324688.48\n  Epoch 9700, Loss: 3449974032.05\n  Epoch 9800, Loss: 2513132819.11\n  Epoch 9900, Loss: 438374038.63\n  Epoch 10000, Loss: 506989264.07\n  Epoch 10100, Loss: 688683847.08\n  Epoch 10200, Loss: 552147666.35\n  Epoch 10300, Loss: 259259844.78\n  Epoch 10400, Loss: 145647404.37\n  Epoch 10500, Loss: 55913462.18\n  Epoch 10600, Loss: 1364518927.26\n  Epoch 10700, Loss: 738200978.66\n  Epoch 10800, Loss: 75121085.75\n  Epoch 10900, Loss: 1052203344.38\n  Epoch 11000, Loss: 7841330705.14\n  Epoch 11100, Loss: 26684207.90\n  Epoch 11200, Loss: 118825577.24\n  Epoch 11300, Loss: 1345098775.67\n  Epoch 11400, Loss: 1212419.43\n  Epoch 11500, Loss: 1764988809.39\n  Epoch 11600, Loss: 3083764754.97\n  Epoch 11700, Loss: 807431811.71\n  Epoch 11800, Loss: 44314743.77\n  Epoch 11900, Loss: 334998866.13\n  Epoch 12000, Loss: 138361923.15\n  Epoch 12100, Loss: 13567169561.76\n  Epoch 12200, Loss: 1471750798.54\n  Epoch 12300, Loss: 225384431.65\n  Epoch 12400, Loss: 55393885.42\n  Epoch 12500, Loss: 559550289.36\n  Epoch 12600, Loss: 83770661.61\n  Epoch 12700, Loss: 497327787.17\n  Epoch 12800, Loss: 2064822288.44\n  Epoch 12900, Loss: 186627277.85\n  Epoch 13000, Loss: 83253984.92\n  Epoch 13100, Loss: 185190052.35\n  Epoch 13200, Loss: 534964148.65\n  Epoch 13300, Loss: 171363605.66\n  Epoch 13400, Loss: 546007818.60\n  Epoch 13500, Loss: 514671121.46\n  Epoch 13600, Loss: 445235387.73\n  Epoch 13700, Loss: 400575454.01\n  Epoch 13800, Loss: 39274.99\n  Epoch 13900, Loss: 56318232.38\n  Epoch 14000, Loss: 166160417.39\n  Epoch 14100, Loss: 151505378.50\n  Epoch 14200, Loss: 1975667863.01\n  Epoch 14300, Loss: 1971333901.53\n  Epoch 14400, Loss: 198473204.78\n  Epoch 14500, Loss: 192268346.78\n  Epoch 14600, Loss: 106062754.03\n  Epoch 14700, Loss: 59371788.96\n  Epoch 14800, Loss: 69791158.04\n  Epoch 14900, Loss: 4820144.46\n  Epoch 15000, Loss: 13450044.41\n  Epoch 15100, Loss: 1670685190.10\n  Epoch 15200, Loss: 8836903.32\n  Epoch 15300, Loss: 6425504.84\n  Epoch 15400, Loss: 5951152.76\n  Epoch 15500, Loss: 1186939.57\n  Epoch 15600, Loss: 36557007.58\n  Epoch 15700, Loss: 9000159.44\n  Epoch 15800, Loss: 372561135.35\n  Epoch 15900, Loss: 102252.45\n  Epoch 16000, Loss: 11492126.48\n  Epoch 16100, Loss: 1954545810.96\n  Epoch 16200, Loss: 2898613010.67\n  Epoch 16300, Loss: 293341264.47\n  Epoch 16400, Loss: 109121310.79\n  Epoch 16500, Loss: 103986.92\n  Epoch 16600, Loss: 216666412.42\n  Epoch 16700, Loss: 29398728.79\n  Epoch 16800, Loss: 4592175.07\n  Epoch 16900, Loss: 1744402.38\n  Epoch 17000, Loss: 6338252.00\n  Epoch 17100, Loss: 11095854.43\n  Epoch 17200, Loss: 46176.12\n  Epoch 17300, Loss: 841131.37\n  Epoch 17400, Loss: 38208.82\n  Epoch 17500, Loss: 143007785.58\n  Epoch 17600, Loss: 10433642.02\n  Epoch 17700, Loss: 1086357.89\n  Epoch 17800, Loss: 572165.01\n  Epoch 17900, Loss: 885422.86\n  Epoch 18000, Loss: 610276.59\n  Epoch 18100, Loss: 26307700.94\n  Epoch 18200, Loss: 143962.51\n  Epoch 18300, Loss: 30244509.23\n  Epoch 18400, Loss: 634239.02\n  Epoch 18500, Loss: 118311724.23\n  Epoch 18600, Loss: 25702041.37\n  Epoch 18700, Loss: 62524395.53\n  Epoch 18800, Loss: 739063.06\n  Epoch 18900, Loss: 2927346.45\n  Epoch 19000, Loss: 5847005.84\n  Epoch 19100, Loss: 772441.14\n  Epoch 19200, Loss: 564638.28\n  Epoch 19300, Loss: 4338929.05\n  Epoch 19400, Loss: 1396736.70\n  Epoch 19500, Loss: 10326.52\n  Epoch 19600, Loss: 24528.71\n  Epoch 19700, Loss: 1217904.38\n  Epoch 19800, Loss: 602780.29\n  Epoch 19900, Loss: 30789801.05\n  Epoch 20000, Loss: 810621.18\n  Epoch 20100, Loss: 38766480.69\n  Epoch 20200, Loss: 47214.84\n  Epoch 20300, Loss: 73812.43\n  Epoch 20400, Loss: 108326.50\n  Epoch 20500, Loss: 18263716.88\n  Epoch 20600, Loss: 6991700.39\n  Epoch 20700, Loss: 5447501.15\n  Epoch 20800, Loss: 173092.05\n  Epoch 20900, Loss: 28848095.23\n  Epoch 21000, Loss: 451082.39\n  Epoch 21100, Loss: 5083558.85\n  Epoch 21200, Loss: 42578.04\n  Epoch 21300, Loss: -9265.14\n  Epoch 21400, Loss: 93546.48\n  Epoch 21500, Loss: 156463.94\n  Epoch 21600, Loss: 206265.91\n  Epoch 21700, Loss: 178718.84\n  Epoch 21800, Loss:  1262.05\n  Epoch 21900, Loss: 32316.24\n  Epoch 22000, Loss: 11384195.64\n  Epoch 22100, Loss: 22635186.68\n  Epoch 22200, Loss: 12027290.62\n  Epoch 22300, Loss: 22929330.49\n  Epoch 22400, Loss: 191643500.19\n  Epoch 22500, Loss: 238656.77\n  Epoch 22600, Loss: 39503.31\n  Epoch 22700, Loss: 86329379.15\n  Epoch 22800, Loss: 69775.27\n  Epoch 22900, Loss: 8877578.62\n  Epoch 23000, Loss: 147304.51\n  Epoch 23100, Loss: 124630.12\n  Epoch 23200, Loss: 84083.92\n  Epoch 23300, Loss: 282453.35\n  Epoch 23400, Loss: 923976.91\n  Epoch 23500, Loss: 25134.00\n  Epoch 23600, Loss: 76824.39\n  Epoch 23700, Loss: 6562636.46\n  Epoch 23800, Loss: 207502.98\n  Epoch 23900, Loss: 500781.82\n  Epoch 24000, Loss: 21917.36\n  Epoch 24100, Loss: 45041.12\n  Epoch 24200, Loss: 129344.81\n  Epoch 24300, Loss: 62005.06\n  Epoch 24400, Loss: 48145.86\n  Epoch 24500, Loss:  9600.52\n  Epoch 24600, Loss: 11688112.30\n  Epoch 24700, Loss: 120259.68\n  Epoch 24800, Loss: 607294.64\n  Epoch 24900, Loss: 30424296.35\n  Epoch 25000, Loss: 1989481.05\n  Epoch 25100, Loss: 20077.54\n  Epoch 25200, Loss: 22573.25\n  Epoch 25300, Loss: 236529.27\n  Epoch 25400, Loss: 509961747.76\n  Epoch 25500, Loss: 22055772.62\n  Epoch 25600, Loss: 863289.91\n  Epoch 25700, Loss: 24699905.82\n  Epoch 25800, Loss: 681747.90\n  Epoch 25900, Loss:  -487.93\n  Epoch 26000, Loss: 11653741.75\n  Epoch 26100, Loss: 279768237.52\n  Epoch 26200, Loss:  8257.83\n  Epoch 26300, Loss: 165594.47\n  Epoch 26400, Loss: 2250215.04\n  Epoch 26500, Loss: 115457.22\n  Epoch 26600, Loss: 44233.33\n  Epoch 26700, Loss: 131155.85\n  Epoch 26800, Loss: 410740244.24\n  Epoch 26900, Loss: 16476.81\n  Epoch 27000, Loss: 3292068.94\n  Epoch 27100, Loss: 702762.70\n  Epoch 27200, Loss: 123205.30\n  Epoch 27300, Loss: 111050.49\n  Epoch 27400, Loss: 220007.59\n  Epoch 27500, Loss:   152.99\n  Epoch 27600, Loss: 710503635.47\n  Epoch 27700, Loss: 27469.78\n  Epoch 27800, Loss: 1118759.70\n  Epoch 27900, Loss:  1371.46\n  Epoch 28000, Loss: 212793.99\n  Epoch 28100, Loss: 198567.77\n  Epoch 28200, Loss: 279590.72\n  Epoch 28300, Loss: 175685.72\n  Epoch 28400, Loss: 344173.52\n  Epoch 28500, Loss: 73470.26\n  Epoch 28600, Loss: 79905.84\n  Epoch 28700, Loss: 286675.24\n  Epoch 28800, Loss: 128325.14\n  Epoch 28900, Loss: 74884.71\n  Epoch 29000, Loss: -5975.58\n  Epoch 29100, Loss: 124931.87\n  Epoch 29200, Loss: 954632.89\n  Epoch 29300, Loss: 2549305.36\n  Epoch 29400, Loss: 81839.45\n  Epoch 29500, Loss: 2241941.22\n  Epoch 29600, Loss: 348890.35\n  Epoch 29700, Loss: 238694.74\n  Epoch 29800, Loss: -9281.75\n  Epoch 29900, Loss: 74849.57\n  Epoch 30000, Loss: 15222.10\n  Epoch 30100, Loss: 132551.76\n  Epoch 30200, Loss: 4398977.73\n  Epoch 30300, Loss: 86995.38\n  Epoch 30400, Loss: -9663.60\n  Epoch 30500, Loss: 69467.70\n  Epoch 30600, Loss: 27709.17\n  Epoch 30700, Loss: 143932.88\n  Epoch 30800, Loss: 14514.55\n  Epoch 30900, Loss: 366346.19\n  Epoch 31000, Loss: 479224.10\n  Epoch 31100, Loss: 245134.95\n  Epoch 31200, Loss: -9722.78\n  Epoch 31300, Loss: 652201.69\n  Epoch 31400, Loss: 77738.63\n  Epoch 31500, Loss: 999229.89\n  Epoch 31600, Loss: 799191.34\n  Epoch 31700, Loss: 4777748.60\n  Epoch 31800, Loss: 101743.62\n  Epoch 31900, Loss: 317968.81\n  Epoch 32000, Loss: 336689.78\n  Epoch 32100, Loss: 315178.00\n  Epoch 32200, Loss: 35365951.88\n  Epoch 32300, Loss: 40842.98\n  Epoch 32400, Loss: 340439.48\n  Epoch 32500, Loss: 34274.63\n  Epoch 32600, Loss: -2371.11\n  Epoch 32700, Loss: 124509.97\n  Epoch 32800, Loss: 15972367.57\n  Epoch 32900, Loss: 60668259.80\n  Epoch 33000, Loss: 15884.83\n  Epoch 33100, Loss: 1465216.17\n  Epoch 33200, Loss: 104770.46\n  Epoch 33300, Loss: 107926.46\n  Epoch 33400, Loss: 928936.21\n  Epoch 33500, Loss: 13940.48\n  Epoch 33600, Loss: 80321.93\n  Epoch 33700, Loss: 7855582.90\n  Epoch 33800, Loss: 121288.09\n  Epoch 33900, Loss: 372186.51\n  Epoch 34000, Loss: 44863.25\n  Epoch 34100, Loss: 98853.89\n  Epoch 34200, Loss: 724936.60\n  Epoch 34300, Loss: 19336602.99\n  Epoch 34400, Loss: 44300.46\n  Epoch 34500, Loss: 64246.29\n  Epoch 34600, Loss: 55726.60\n  Epoch 34700, Loss: 1465169.97\n  Epoch 34800, Loss: 152793.00\n  Epoch 34900, Loss: 54254.57\n  Epoch 35000, Loss: 170568.61\n  Epoch 35100, Loss: 164820.29\n  Epoch 35200, Loss: 376791.35\n  Epoch 35300, Loss: 98548.42\n  Epoch 35400, Loss: -14163.07\n  Epoch 35500, Loss: 38968.16\n  Epoch 35600, Loss:  3750.46\n  Epoch 35700, Loss: 336227.37\n  Epoch 35800, Loss: 112657.40\n  Epoch 35900, Loss: 51263.46\n  Epoch 36000, Loss: -1594.56\n  Epoch 36100, Loss: 22321.07\n  Epoch 36200, Loss:  7630.97\n  Epoch 36300, Loss: 235241.69\n  Epoch 36400, Loss: 152526.81\n  Epoch 36500, Loss: 43431.08\n  Epoch 36600, Loss: 183994.93\n  Epoch 36700, Loss: 30902.14\n  Epoch 36800, Loss: 101840.59\n  Epoch 36900, Loss: 141057.78\n  Epoch 37000, Loss: 29941.71\n  Epoch 37100, Loss: 14506396.90\n  Epoch 37200, Loss: 356264.81\n  Epoch 37300, Loss: 62068.26\n  Epoch 37400, Loss: -9797.19\n  Epoch 37500, Loss: 1443238.83\n  Epoch 37600, Loss: 14875.02\n  Epoch 37700, Loss: 164880.28\n  Epoch 37800, Loss: 226577.00\n  Epoch 37900, Loss: 20126.07\n  Epoch 38000, Loss: 140281.22\n  Epoch 38100, Loss: 16283.83\n  Epoch 38200, Loss: 148717.31\n  Epoch 38300, Loss: 300411.35\n  Epoch 38400, Loss: 32395.08\n  Epoch 38500, Loss: 26233507.65\n  Epoch 38600, Loss: -3894.15\n  Epoch 38700, Loss: 13743.79\n  Epoch 38800, Loss: -10964.88\n  Epoch 38900, Loss: -12466.94\n  Epoch 39000, Loss: -15075.51\n  Epoch 39100, Loss: 70195.88\n  Epoch 39200, Loss: 202873.60\n  Epoch 39300, Loss: 329162.21\n  Epoch 39400, Loss: 34431.61\n  Epoch 39500, Loss: 472577.46\n  Epoch 39600, Loss:  -391.09\n  Epoch 39700, Loss: 160686.64\n  Epoch 39800, Loss: 156768.67\n  Epoch 39900, Loss: -1260.14\n  Epoch 40000, Loss: 85190.09\n  Epoch 40100, Loss: 10018.18\n  Epoch 40200, Loss: 720069.51\n  Epoch 40300, Loss: -7520.44\n  Epoch 40400, Loss: 155111.54\n  Epoch 40500, Loss: -1615.71\n  Epoch 40600, Loss: 61762.65\n  Epoch 40700, Loss: -3393.69\n  Epoch 40800, Loss: 369941.28\n  Epoch 40900, Loss: 41312.95\n  Epoch 41000, Loss: -4976.31\n  Epoch 41100, Loss: 25906.05\n  Epoch 41200, Loss: 120522.15\n  Epoch 41300, Loss: 359020.35\n  Epoch 41400, Loss: -15102.93\n  Epoch 41500, Loss: 39141.21\n  Epoch 41600, Loss: 12937.03\n  Epoch 41700, Loss:  9793.79\n  Epoch 41800, Loss: 97642.51\n  Epoch 41900, Loss:  3545.84\n  Epoch 42000, Loss:  7612.02\n  Epoch 42100, Loss: 17336.04\n  Epoch 42200, Loss: -15665.99\n  Epoch 42300, Loss: 29232.58\n  Epoch 42400, Loss: -3766.69\n  Epoch 42500, Loss: 121385.07\n  Epoch 42600, Loss:   838.22\n  Epoch 42700, Loss: -10523.40\n  Epoch 42800, Loss: 388136.71\n  Epoch 42900, Loss: 22546.24\n  Epoch 43000, Loss: 164840.49\n  Epoch 43100, Loss: -11253.34\n  Epoch 43200, Loss: 28593.50\n  Epoch 43300, Loss: -7026.07\n  Epoch 43400, Loss:  1313.92\n  Epoch 43500, Loss: 86332.19\n  Epoch 43600, Loss: 63105.85\n  Epoch 43700, Loss:  5285.63\n  Epoch 43800, Loss: -13872.90\n  Epoch 43900, Loss: 66120.56\n  Epoch 44000, Loss:  3111.50\n  Epoch 44100, Loss: 17141.02\n  Epoch 44200, Loss: 11260.18\n  Epoch 44300, Loss: 37934.99\n  Epoch 44400, Loss:  4368.70\n  Epoch 44500, Loss: -14138.76\n  Epoch 44600, Loss: 144825.32\n  Epoch 44700, Loss: 102564.51\n  Epoch 44800, Loss: -17583.53\n  Epoch 44900, Loss: 49165.30\n  Epoch 45000, Loss: -16354.68\n  Epoch 45100, Loss: -8070.39\n  Epoch 45200, Loss: 415258.02\n  Epoch 45300, Loss:  -873.21\n  Epoch 45400, Loss: 108569.19\n  Epoch 45500, Loss: 5210910.06\n  Epoch 45600, Loss: -5222.24\n  Epoch 45700, Loss: -13051.52\n  Epoch 45800, Loss: -12868.94\n  Epoch 45900, Loss: -3305.84\n  Epoch 46000, Loss: 65314.54\n  Epoch 46100, Loss: -8419.18\n  Epoch 46200, Loss: -8734.92\n  Epoch 46300, Loss: -14221.63\n  Epoch 46400, Loss: 28413.98\n  Epoch 46500, Loss: 48576.66\n  Epoch 46600, Loss: -3702.36\n  Epoch 46700, Loss:   465.28\n  Epoch 46800, Loss: -10714.22\n  Epoch 46900, Loss: 6714825.06\n  Epoch 47000, Loss: -1024.79\n  Epoch 47100, Loss: 22621.82\n  Epoch 47200, Loss:   143.65\n  Epoch 47300, Loss:  7608.08\n  Epoch 47400, Loss:   386.12\n  Epoch 47500, Loss: 19155.76\n  Epoch 47600, Loss: -4093.93\n  Epoch 47700, Loss: 214165.77\n  Epoch 47800, Loss: -4576.39\n  Epoch 47900, Loss: 14857.61\n  Epoch 48000, Loss: -16414.95\n  Epoch 48100, Loss: -14798.14\n  Epoch 48200, Loss: -11952.68\n  Epoch 48300, Loss:  8269.19\n  Epoch 48400, Loss:  9367.70\n  Epoch 48500, Loss: -18041.00\n  Epoch 48600, Loss: -13414.78\n  Epoch 48700, Loss: 152033.37\n  Epoch 48800, Loss: -19616.24\n  Epoch 48900, Loss: 37091.46\n  Epoch 49000, Loss: -1474.60\n  Epoch 49100, Loss: 20064.16\n  Epoch 49200, Loss: 181772.72\n  Epoch 49300, Loss: 104888.73\n  Epoch 49400, Loss: -3982.23\n  Epoch 49500, Loss: -16706.44\n  Epoch 49600, Loss: 42310.54\n  Epoch 49700, Loss:  1349.87\n  Epoch 49800, Loss: 74319.04\n  Epoch 49900, Loss: -15811.56\n  Epoch 50000, Loss: 44147.32\n  Epoch 50100, Loss: -15645.75\n  Epoch 50200, Loss: 11081.75\n  Epoch 50300, Loss: 50872.24\n  Epoch 50400, Loss: -18009.45\n  Epoch 50500, Loss:  8503.15\n  Epoch 50600, Loss: -15299.43\n  Epoch 50700, Loss: -19919.49\n  Epoch 50800, Loss: 88816.21\n  Epoch 50900, Loss: -20341.22\n  Epoch 51000, Loss: 29323.40\n  Epoch 51100, Loss: 14507.66\n  Epoch 51200, Loss: -2898.03\n  Epoch 51300, Loss: 23038.32\n  Epoch 51400, Loss: -19809.20\n  Epoch 51500, Loss: -13975.90\n  Epoch 51600, Loss: -21052.82\n  Epoch 51700, Loss: -21332.57\n  Epoch 51800, Loss: -3382.45\n  Epoch 51900, Loss: -16946.32\n  Epoch 52000, Loss: 47735.37\n  Epoch 52100, Loss: -18644.62\n  Epoch 52200, Loss: -19876.30\n  Epoch 52300, Loss: 145538.91\n  Epoch 52400, Loss: -15511.76\n  Epoch 52500, Loss: -20064.73\n  Epoch 52600, Loss: -19805.46\n  Epoch 52700, Loss: -21051.83\n  Epoch 52800, Loss: -21241.09\n  Epoch 52900, Loss: 80337.58\n  Epoch 53000, Loss: -11760.00\n  Epoch 53100, Loss:  6304.63\n  Epoch 53200, Loss: -15513.90\n  Epoch 53300, Loss: -20958.60\n  Epoch 53400, Loss: 57359.21\n  Epoch 53500, Loss: -19017.92\n  Epoch 53600, Loss:   847.53\n  Epoch 53700, Loss: -20523.80\n  Epoch 53800, Loss:  8483.11\n  Epoch 53900, Loss: 11506.31\n  Epoch 54000, Loss: -4766.45\n  Epoch 54100, Loss: -14938.69\n  Epoch 54200, Loss: 23494.04\n  Epoch 54300, Loss: -12263.50\n  Epoch 54400, Loss: -14110.01\n  Epoch 54500, Loss: -21871.98\n  Epoch 54600, Loss: -17943.95\n  Epoch 54700, Loss: -21496.34\n  Epoch 54800, Loss: -13712.64\n  Epoch 54900, Loss: -14955.02\n  Epoch 55000, Loss: 3596185.07\n  Epoch 55100, Loss: -16501.69\n  Epoch 55200, Loss: -21310.38\n  Epoch 55300, Loss: -15543.55\n  Epoch 55400, Loss: -4434.61\n  Epoch 55500, Loss: -21061.79\n  Epoch 55600, Loss: -3380.64\n  Epoch 55700, Loss: -21139.42\n  Epoch 55800, Loss: -22452.22\n  Epoch 55900, Loss: -16271.29\n  Epoch 56000, Loss: -2046.76\n  Epoch 56100, Loss: 13195886.54\n  Epoch 56200, Loss: 27401.45\n  Epoch 56300, Loss: -20933.51\n  Epoch 56400, Loss: 13555.94\n  Epoch 56500, Loss: -22155.65\n  Epoch 56600, Loss: -4535.58\n  Epoch 56700, Loss: -22762.31\n  Epoch 56800, Loss: -22724.02\n  Epoch 56900, Loss: -23298.64\n  Epoch 57000, Loss: -12339.02\n  Epoch 57100, Loss: -22403.20\n  Epoch 57200, Loss: 86163841.10\n  Epoch 57300, Loss: -18676.35\n  Epoch 57400, Loss: -24216.43\n  Epoch 57500, Loss: -22580.38\n  Epoch 57600, Loss:  2795.83\n  Epoch 57700, Loss: -5181.40\n  Epoch 57800, Loss: -20726.54\n  Epoch 57900, Loss: -24602.52\n  Epoch 58000, Loss: -21642.56\n  Epoch 58100, Loss: -11336.25\n  Epoch 58200, Loss: -25140.75\n  Epoch 58300, Loss: -24239.39\n  Epoch 58400, Loss: -24744.18\n  Epoch 58500, Loss: -24963.59\n  Epoch 58600, Loss: 7311774.74\n  Epoch 58700, Loss: -20741.08\n  Epoch 58800, Loss: -20336.66\n  Epoch 58900, Loss: -12152.39\n  Epoch 59000, Loss: -20699.36\n  Epoch 59100, Loss: -20093.93\n  Epoch 59200, Loss: -22890.73\n  Epoch 59300, Loss: -23780.91\n  Epoch 59400, Loss: -22347.80\n  Epoch 59500, Loss: -22496.71\n  Epoch 59600, Loss: -23458.11\n  Epoch 59700, Loss: -21846.79\n  Epoch 59800, Loss: -24958.15\n  Epoch 59900, Loss: -24598.54\n  Epoch 60000, Loss: -21910.17\n  Epoch 60100, Loss: -23091.11\n  Epoch 60200, Loss: -22160.15\n  Epoch 60300, Loss: -21721.44\n  Epoch 60400, Loss: -17962.74\n  Epoch 60500, Loss: -22109.31\n  Epoch 60600, Loss: -18228.68\n  Epoch 60700, Loss: -23241.50\n  Epoch 60800, Loss: -24522.27\n  Epoch 60900, Loss: -25692.67\n  Epoch 61000, Loss:   312.45\n  Epoch 61100, Loss: -24700.83\n  Epoch 61200, Loss: -13222.80\n  Epoch 61300, Loss: -17651.41\n  Epoch 61400, Loss: -26802.33\n  Epoch 61500, Loss: -15676.34\n  Epoch 61600, Loss: -25861.89\n  Epoch 61700, Loss: -23577.99\n  Epoch 61800, Loss: -26813.39\n  Epoch 61900, Loss: 10550.70\n  Epoch 62000, Loss: -27038.06\n  Epoch 62100, Loss: -25858.00\n  Epoch 62200, Loss: -24852.93\n  Epoch 62300, Loss: -26406.88\n  Epoch 62400, Loss: -25884.86\n  Epoch 62500, Loss: -24280.16\n  Epoch 62600, Loss: -23897.23\n  Epoch 62700, Loss: -13118.45\n  Epoch 62800, Loss: -22679.67\n  Epoch 62900, Loss: -16226.26\n  Epoch 63000, Loss: -26127.50\n  Epoch 63100, Loss: -24711.70\n  Epoch 63200, Loss: -25289.62\n  Epoch 63300, Loss: -20951.68\n  Epoch 63400, Loss: -26736.21\n  Epoch 63500, Loss: -22239.02\n  Epoch 63600, Loss: -24370.06\n  Epoch 63700, Loss: 1882184.98\n  Epoch 63800, Loss: -26420.32\n  Epoch 63900, Loss: -26260.80\n  Epoch 64000, Loss: -26976.89\n  Epoch 64100, Loss: -19873.12\n  Epoch 64200, Loss: -22245.93\n  Epoch 64300, Loss: -23026.85\n  Epoch 64400, Loss: -27937.68\n  Epoch 64500, Loss: -25475.39\n  Epoch 64600, Loss: -27926.99\n  Epoch 64700, Loss: -24213.50\n  Epoch 64800, Loss: -29871.01\n  Epoch 64900, Loss: -22336.32\n  Epoch 65000, Loss: -27078.38\n  Epoch 65100, Loss:  9379.27\n  Epoch 65200, Loss: -29121.17\n  Epoch 65300, Loss: -17909.62\n  Epoch 65400, Loss: 74478.62\n  Epoch 65500, Loss: -22745.27\n  Epoch 65600, Loss: -28126.59\n  Epoch 65700, Loss: -21020.43\n  Epoch 65800, Loss: -26793.12\n  Epoch 65900, Loss: -24627.40\n  Epoch 66000, Loss: -25980.72\n  Epoch 66100, Loss: -20071.44\n  Epoch 66200, Loss: -21847.15\n  Epoch 66300, Loss: 11915.62\n  Epoch 66400, Loss: -28103.26\n  Epoch 66500, Loss: -26435.02\n  Epoch 66600, Loss: -19348.10\n  Epoch 66700, Loss: -24134.50\n  Epoch 66800, Loss: -27498.15\n  Epoch 66900, Loss: -25795.27\n  Epoch 67000, Loss: -24829.30\n  Epoch 67100, Loss: -28287.00\n  Epoch 67200, Loss: -25024.36\n  Epoch 67300, Loss: -26049.04\n  Epoch 67400, Loss: -24728.61\n  Epoch 67500, Loss: -26208.03\n  Epoch 67600, Loss: -28908.06\n  Epoch 67700, Loss: -23617.45\n  Epoch 67800, Loss: -26031.50\n  Epoch 67900, Loss: -25286.71\n  Epoch 68000, Loss: -22915.34\n  Epoch 68100, Loss: -24093.66\n  Epoch 68200, Loss: -27379.60\n  Epoch 68300, Loss: 118460.74\n  Epoch 68400, Loss: -25615.11\n  Epoch 68500, Loss: -29249.55\n  Epoch 68600, Loss: -28274.07\n  Epoch 68700, Loss: -23856.82\n  Epoch 68800, Loss: -28493.92\n  Epoch 68900, Loss:   882.25\n  Epoch 69000, Loss: -11604.81\n  Epoch 69100, Loss: -25849.08\n  Epoch 69200, Loss: -23070.46\n  Epoch 69300, Loss: -22493.92\n  Epoch 69400, Loss: -23455.23\n  Epoch 69500, Loss: -28729.77\n  Epoch 69600, Loss: -29581.52\n  Epoch 69700, Loss: -24583.38\n  Epoch 69800, Loss: -26333.53\n  Epoch 69900, Loss: -26003.32\n  Epoch 70000, Loss: -26703.58\n  Epoch 70100, Loss: -24771.96\n  Epoch 70200, Loss: -19829.01\n  Epoch 70300, Loss: -27021.10\n  Epoch 70400, Loss: -21006.87\n  Epoch 70500, Loss: -26277.54\n  Epoch 70600, Loss: -25910.97\n  Epoch 70700, Loss: -22091.06\n  Epoch 70800, Loss: -25039.15\n  Epoch 70900, Loss: 86169.38\n  Epoch 71000, Loss: -28849.04\n  Epoch 71100, Loss: -26016.07\n  Epoch 71200, Loss: -27594.93\n  Epoch 71300, Loss: -30893.95\n  Epoch 71400, Loss: -27104.43\n  Epoch 71500, Loss: -24192.91\n  Epoch 71600, Loss: -23378.47\n  Epoch 71700, Loss: -28932.48\n  Epoch 71800, Loss: -30024.46\n  Epoch 71900, Loss: -26153.08\n  Epoch 72000, Loss: 50069.11\n  Epoch 72100, Loss: -29750.10\n  Epoch 72200, Loss: -29365.25\n  Epoch 72300, Loss:  7128.24\n  Epoch 72400, Loss: -28931.97\n  Epoch 72500, Loss: -24495.41\n  Epoch 72600, Loss: -26369.55\n  Epoch 72700, Loss: -29314.38\n  Epoch 72800, Loss: -27972.83\n  Epoch 72900, Loss: -29035.70\n  Epoch 73000, Loss: -24804.77\n  Epoch 73100, Loss: -25726.78\n  Epoch 73200, Loss: -29594.58\n  Epoch 73300, Loss: -21166.67\n  Epoch 73400, Loss: -23857.47\n  Epoch 73500, Loss:   167.38\n  Epoch 73600, Loss: -31177.44\n  Epoch 73700, Loss: -27884.40\n  Epoch 73800, Loss: -30502.24\n  Epoch 73900, Loss: -26754.37\n  Epoch 74000, Loss: 32625.70\n  Epoch 74100, Loss: -29430.24\n  Epoch 74200, Loss: -29208.99\n  Epoch 74300, Loss: -25470.16\n  Epoch 74400, Loss: -24074.87\n  Epoch 74500, Loss: -23574.95\n  Epoch 74600, Loss: -26034.77\n  Epoch 74700, Loss: -26031.80\n  Epoch 74800, Loss: -24620.56\n  Epoch 74900, Loss: -25591.52\n  Epoch 75000, Loss: -26429.01\n  Epoch 75100, Loss: -28830.06\n  Epoch 75200, Loss: -30522.71\n  Epoch 75300, Loss: -29252.14\n  Epoch 75400, Loss: -27909.91\n  Epoch 75500, Loss: -28189.24\n  Epoch 75600, Loss: -26509.72\n  Epoch 75700, Loss: -24194.18\n  Epoch 75800, Loss: -27971.00\n  Epoch 75900, Loss: -25671.56\n  Epoch 76000, Loss: -26406.65\n  Epoch 76100, Loss: -24485.89\n  Epoch 76200, Loss: -27184.24\n  Epoch 76300, Loss: -23639.98\n  Epoch 76400, Loss: -15416.62\n  Epoch 76500, Loss: -27955.08\n  Epoch 76600, Loss: -27729.67\n  Epoch 76700, Loss: -28963.91\n  Epoch 76800, Loss: -25483.71\n  Epoch 76900, Loss: -11245.27\n  Epoch 77000, Loss: -25877.70\n  Epoch 77100, Loss: -22449.00\n  Epoch 77200, Loss: 16327.33\n  Epoch 77300, Loss: -24723.05\n  Epoch 77400, Loss: -30925.98\n  Epoch 77500, Loss: -30829.83\n  Epoch 77600, Loss: -25345.62\n  Epoch 77700, Loss: -17420.50\n  Epoch 77800, Loss: -30714.53\n  Epoch 77900, Loss: -29976.53\n  Epoch 78000, Loss: -18719.92\n  Epoch 78100, Loss: -28400.46\n  Epoch 78200, Loss: -28801.54\n  Epoch 78300, Loss: -25246.39\n  Epoch 78400, Loss: -25120.42\n  Epoch 78500, Loss: -29989.20\n  Epoch 78600, Loss: -28872.84\n  Epoch 78700, Loss: -18122.08\n  Epoch 78800, Loss: -26187.52\n  Epoch 78900, Loss: -27533.54\n  Epoch 79000, Loss: -22737.31\n  Epoch 79100, Loss: -29812.00\n  Epoch 79200, Loss: -25868.41\n  Epoch 79300, Loss: -19991.51\n  Epoch 79400, Loss: -27483.14\n  Epoch 79500, Loss: -30187.19\n  Epoch 79600, Loss: -26663.04\n  Epoch 79700, Loss: -25370.87\n  Epoch 79800, Loss: -23786.19\n  Epoch 79900, Loss: -27125.98\n  Epoch 80000, Loss: -30309.41\n  Epoch 80100, Loss: -22435.45\n  Epoch 80200, Loss: -25993.94\n  Epoch 80300, Loss: -28675.77\n  Epoch 80400, Loss: -28348.18\n  Epoch 80500, Loss: -25617.38\n  Epoch 80600, Loss: -25711.46\n  Epoch 80700, Loss: -23404.15\n  Epoch 80800, Loss: -29527.96\n  Epoch 80900, Loss: -28090.54\n  Epoch 81000, Loss: -23720.15\n  Epoch 81100, Loss: -30043.44\n  Epoch 81200, Loss: -30132.02\n  Epoch 81300, Loss: -13605.27\n  Epoch 81400, Loss: -28405.18\n  Epoch 81500, Loss: -23396.12\n  Epoch 81600, Loss: -22022.73\n  Epoch 81700, Loss: -29113.50\n  Epoch 81800, Loss: -27847.58\n  Epoch 81900, Loss: -12355.76\n  Epoch 82000, Loss: -22193.01\n  Epoch 82100, Loss: -28245.95\n  Epoch 82200, Loss: -23107.36\n  Epoch 82300, Loss: -24199.39\n  Epoch 82400, Loss: -28888.81\n  Epoch 82500, Loss: -28212.19\n  Epoch 82600, Loss: -24772.96\n  Epoch 82700, Loss: -27888.76\n  Epoch 82800, Loss: -28214.96\n  Epoch 82900, Loss: -27962.55\n  Epoch 83000, Loss: -26985.10\n  Epoch 83100, Loss: -30574.43\n  Epoch 83200, Loss: -26499.77\n  Epoch 83300, Loss: -28525.58\n  Epoch 83400, Loss: -28884.95\n  Epoch 83500, Loss: -24007.01\n  Epoch 83600, Loss: -29132.58\n  Epoch 83700, Loss: -26754.47\n  Epoch 83800, Loss: -25458.91\n  Epoch 83900, Loss: -17268.82\n  Epoch 84000, Loss: -24622.96\n  Epoch 84100, Loss: -31082.51\n  Epoch 84200, Loss: -26647.97\n  Epoch 84300, Loss: -27599.25\n  Epoch 84400, Loss: -28352.45\n  Epoch 84500, Loss: -28606.12\n  Epoch 84600, Loss: -29611.27\n  Epoch 84700, Loss: -26638.55\n  Epoch 84800, Loss: -25921.56\n  Epoch 84900, Loss: -27437.46\n  Epoch 85000, Loss: -29329.63\n  Epoch 85100, Loss: -26043.02\n  Epoch 85200, Loss: -23260.87\n  Epoch 85300, Loss: -22289.44\n  Epoch 85400, Loss: -18822.17\n  Epoch 85500, Loss: -28029.29\n  Epoch 85600, Loss: -23235.03\n  Epoch 85700, Loss:  -401.13\n  Epoch 85800, Loss: -25710.72\n  Epoch 85900, Loss: -22848.63\n  Epoch 86000, Loss: -16191.75\n  Epoch 86100, Loss: -19976.70\n  Epoch 86200, Loss: -27115.66\n  Epoch 86300, Loss: -28309.66\n  Epoch 86400, Loss: -28058.14\n  Epoch 86500, Loss: -29023.53\n  Epoch 86600, Loss: -30934.32\n  Epoch 86700, Loss: -21731.42\n  Epoch 86800, Loss: -27387.34\n  Epoch 86900, Loss: -29538.89\n  Epoch 87000, Loss: -24999.86\n  Epoch 87100, Loss: -25151.68\n  Epoch 87200, Loss: -30335.29\n  Epoch 87300, Loss: -27180.06\n  Epoch 87400, Loss: -26393.13\n  Epoch 87500, Loss: -25663.08\n  Epoch 87600, Loss: -29080.60\n  Epoch 87700, Loss: -24189.69\n  Epoch 87800, Loss: -20937.11\n  Epoch 87900, Loss: -23070.14\n  Epoch 88000, Loss: -26779.53\n  Epoch 88100, Loss: -27699.01\n  Epoch 88200, Loss: -28737.45\n  Epoch 88300, Loss: -21439.32\n  Epoch 88400, Loss: -25916.61\n  Epoch 88500, Loss: -29492.72\n  Epoch 88600, Loss: -25665.20\n  Epoch 88700, Loss: -30644.64\n  Epoch 88800, Loss: -21505.93\n  Epoch 88900, Loss: -28582.31\n  Epoch 89000, Loss: -24449.48\n  Epoch 89100, Loss: -31259.51\n  Epoch 89200, Loss: -31014.72\n  Epoch 89300, Loss: -28918.73\n  Epoch 89400, Loss: -28317.42\n  Epoch 89500, Loss: -22855.50\n  Epoch 89600, Loss: -14118.98\n  Epoch 89700, Loss: -22261.59\n  Epoch 89800, Loss: -25286.02\n  Epoch 89900, Loss: -29205.32\n  Epoch 90000, Loss: -29836.10\n  Epoch 90100, Loss: -30702.09\n  Epoch 90200, Loss: -27849.21\n  Epoch 90300, Loss: -26270.93\n  Epoch 90400, Loss: 17254.05\n  Epoch 90500, Loss: -24888.67\n  Epoch 90600, Loss: -25062.05\n  Epoch 90700, Loss: -26389.20\n  Epoch 90800, Loss: -25244.83\n  Epoch 90900, Loss: -31281.69\n  Epoch 91000, Loss: -25912.94\n  Epoch 91100, Loss: -29012.40\n  Epoch 91200, Loss: -30747.20\n  Epoch 91300, Loss: -29054.90\n  Epoch 91400, Loss: -29465.64\n  Epoch 91500, Loss: -24791.32\n  Epoch 91600, Loss: -28767.04\n  Epoch 91700, Loss: -29121.35\n  Epoch 91800, Loss: -27921.75\n  Epoch 91900, Loss: -23318.38\n  Epoch 92000, Loss: -26725.26\n  Epoch 92100, Loss: -26281.04\n  Epoch 92200, Loss: -21899.23\n  Epoch 92300, Loss: -31279.60\n  Epoch 92400, Loss: -26106.07\n  Epoch 92500, Loss: -27946.15\n  Epoch 92600, Loss: -26648.28\n  Epoch 92700, Loss: -29276.60\n  Epoch 92800, Loss: -31859.68\n  Epoch 92900, Loss: -24451.06\n  Epoch 93000, Loss: -21894.28\n  Epoch 93100, Loss: -29383.34\n  Epoch 93200, Loss: -27834.25\n  Epoch 93300, Loss: -17969.93\n  Epoch 93400, Loss: -27913.24\n  Epoch 93500, Loss: -27687.82\n  Epoch 93600, Loss: -19147.81\n  Epoch 93700, Loss: -25267.98\n  Epoch 93800, Loss: -26462.38\n  Epoch 93900, Loss: -24885.71\n  Epoch 94000, Loss: -30762.91\n  Epoch 94100, Loss: -29466.80\n  Epoch 94200, Loss: -21454.37\n  Epoch 94300, Loss: -22905.44\n  Epoch 94400, Loss: -24725.15\n  Epoch 94500, Loss: -26214.00\n  Epoch 94600, Loss: -25138.06\n  Epoch 94700, Loss: -27719.25\n  Epoch 94800, Loss: -30145.59\n  Epoch 94900, Loss: -25094.69\n  Epoch 95000, Loss: -27777.22\n  Epoch 95100, Loss: -32750.13\n  Epoch 95200, Loss: -24674.56\n  Epoch 95300, Loss: -25949.03\n  Epoch 95400, Loss: -30095.87\n  Epoch 95500, Loss: -29277.31\n  Epoch 95600, Loss: -31796.92\n  Epoch 95700, Loss: -26213.17\n  Epoch 95800, Loss: -25263.72\n  Epoch 95900, Loss: -29170.84\n  Epoch 96000, Loss: -25923.53\n  Epoch 96100, Loss: -22734.60\n  Epoch 96200, Loss: -31103.17\n  Epoch 96300, Loss: -28416.57\n  Epoch 96400, Loss: -25595.46\n  Epoch 96500, Loss: -26707.66\n  Epoch 96600, Loss: -27804.76\n  Epoch 96700, Loss: -22091.09\n  Epoch 96800, Loss: -30594.26\n  Epoch 96900, Loss: -29199.03\n  Epoch 97000, Loss: -20288.35\n  Epoch 97100, Loss: -29245.94\n  Epoch 97200, Loss: -31305.94\n  Epoch 97300, Loss: -26549.85\n  Epoch 97400, Loss: -29679.93\n  Epoch 97500, Loss: -29142.37\n  Epoch 97600, Loss: -28937.31\n  Epoch 97700, Loss: -27570.62\n  Epoch 97800, Loss: -28663.14\n  Epoch 97900, Loss: -20980.14\n  Epoch 98000, Loss: -25020.29\n  Epoch 98100, Loss: -28793.64\n  Epoch 98200, Loss: -27305.88\n  Epoch 98300, Loss: -24518.27\n  Epoch 98400, Loss: -28322.04\n  Epoch 98500, Loss: -29456.30\n  Epoch 98600, Loss: -27129.54\n  Epoch 98700, Loss: -27502.90\n  Epoch 98800, Loss: -26944.82\n  Epoch 98900, Loss: -28798.09\n  Epoch 99000, Loss: -27980.88\n  Epoch 99100, Loss: -28265.08\n  Epoch 99200, Loss: -29713.16\n  Epoch 99300, Loss: -26560.93\n  Epoch 99400, Loss: -26067.83\n  Epoch 99500, Loss: -19180.68\n  Epoch 99600, Loss: -26127.43\n  Epoch 99700, Loss: -32052.96\n  Epoch 99800, Loss: -25611.30\n  Epoch 99900, Loss: -3981.92\nGenerating validation predictions...\nTraining complete. Final loss: -28118.46\nPyro models saved to: /kaggle/working/ariel-data-2025-27\n\\nTraining complete!\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}